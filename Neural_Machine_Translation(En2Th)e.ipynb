{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-5fS6rg2zLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be49d500-e279-4dba-d37c-8dd40f22aba1",
        "collapsed": true
      },
      "source": [
        "!pip install -q sacrebleu==1.2.10 torch pythainlp==2.1.4 mosestokenizer sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-5 (attachment_entry):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 237, in listen\n",
            "    sock, _ = endpoints_listener.accept()\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 293, in accept\n",
            "    fd, addr = self._accept()\n",
            "TimeoutError: timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/colab/_debugpy.py\", line 52, in attachment_entry\n",
            "    debugpy.listen(_dap_port)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/public_api.py\", line 31, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 143, in debug\n",
            "    log.reraise_exception(\"{0}() failed:\", func.__name__, level=\"info\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 141, in debug\n",
            "    return func(address, settrace_kwargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 251, in listen\n",
            "    raise RuntimeError(\"timed out waiting for adapter to connect\")\n",
            "RuntimeError: timed out waiting for adapter to connect\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fairseq 0.12.2 requires sacrebleu>=1.4.12, but you have sacrebleu 1.2.10 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVtvySBd2zLv",
        "collapsed": true
      },
      "source": [
        "# Note: It is required to install from this commit ID @6f6461b.\n",
        "\n",
        "# !pip install git+https://github.com/pytorch/fairseq@6f6461b"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the specific commit\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "!git checkout 6f6461b\n",
        "\n",
        "# Initialize and update submodules\n",
        "!git submodule update --init --recursive\n",
        "\n",
        "# Install fairseq\n",
        "!pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AySRBflWkdM0",
        "outputId": "4657f00d-f1c3-4085-db4e-172782c0c5b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'fairseq' already exists and is not an empty directory.\n",
            "/content/fairseq\n",
            "M\tfairseq/model_parallel/megatron\n",
            "HEAD is now at 6f6461b8 Add tracepoints (#1192)\n",
            "Cloning into '/content/fairseq/fairseq/models/huggingface/transformers'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "fatal: clone of 'https://github.com/myleott/transformers.git' into submodule path '/content/fairseq/fairseq/models/huggingface/transformers' failed\n",
            "Failed to clone 'fairseq/models/huggingface/transformers'. Retry scheduled\n",
            "Cloning into '/content/fairseq/fairseq/models/huggingface/transformers'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "fatal: clone of 'https://github.com/myleott/transformers.git' into submodule path '/content/fairseq/fairseq/models/huggingface/transformers' failed\n",
            "Failed to clone 'fairseq/models/huggingface/transformers' a second time, aborting\n",
            "Processing /content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (3.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (1.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (2024.5.15)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (1.2.10)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (4.66.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.9.0) (2.22)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.10/dist-packages (from sacrebleu->fairseq==0.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq==0.9.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq==0.9.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq==0.9.0) (1.3.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp310-cp310-linux_x86_64.whl size=1654311 sha256=27f1910b333375fa9b326d620e6283f5b12644d1be9373f51a6bd47161f2b943\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s_02099i/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "Successfully built fairseq\n",
            "Installing collected packages: fairseq\n",
            "  Attempting uninstall: fairseq\n",
            "    Found existing installation: fairseq 0.12.2\n",
            "    Uninstalling fairseq-0.12.2:\n",
            "      Successfully uninstalled fairseq-0.12.2\n",
            "Successfully installed fairseq-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xGpoArqplHRF",
        "outputId": "ba0432ce-d3ff-4d60-f8ae-d769bbbd86da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.21.0)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.0\n",
            "    Uninstalling numpy-1.21.0:\n",
            "      Successfully uninstalled numpy-1.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.0.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.0.0 which is incompatible.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.4.0 which is incompatible.\n",
            "langchain 0.2.6 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.0 which is incompatible.\n",
            "langchain-community 0.2.6 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.0 which is incompatible.\n",
            "numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.0 which is incompatible.\n",
            "onnxruntime 1.18.1 requires numpy<2.0,>=1.21.6, but you have numpy 2.0.0 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n",
            "scipy 1.10.1 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.0.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.0.0 which is incompatible.\n",
            "unstructured 0.14.9 requires numpy<2, but you have numpy 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.float = float  # This is not recommended for long-term use"
      ],
      "metadata": {
        "id": "TkFQIWam4y_m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import fairseq\n",
        "import mosestokenizer\n",
        "import pythainlp\n",
        "\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Fairseq version: {fairseq.__version__}\")\n",
        "print(f\"Mosestokenizer version: {mosestokenizer.__version__}\")\n",
        "print(f\"Pythainlp version: {pythainlp.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH_4T8ximuce",
        "outputId": "eaa6f4ab-0771-4b15-c894-9d18e1ac21d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.21.0\n",
            "Fairseq version: 0.9.0\n",
            "Mosestokenizer version: 1.2.1\n",
            "Pythainlp version: 2.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy fairseq mosestokenizer pythainlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "4vWQB4qOmxsO",
        "outputId": "67ead77c-0776-48f8-f8f6-d88f59bdde12"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Collecting fairseq\n",
            "  Using cached fairseq-0.12.2-cp310-cp310-linux_x86_64.whl\n",
            "Requirement already satisfied: mosestokenizer in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: pythainlp in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Collecting pythainlp\n",
            "  Using cached pythainlp-5.0.4-py3-none-any.whl (17.9 MB)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.10)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2024.5.15)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Using cached sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.4)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.9.2)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.0+cu121)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from mosestokenizer) (0.6.2)\n",
            "Requirement already satisfied: openfile in /usr/local/lib/python3.10/dist-packages (from mosestokenizer) (0.0.7)\n",
            "Requirement already satisfied: uctools in /usr/local/lib/python3.10/dist-packages (from mosestokenizer) (1.3.0)\n",
            "Requirement already satisfied: toolwrapper in /usr/local/lib/python3.10/dist-packages (from mosestokenizer) (2.1.0)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.32.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2024.6.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.10.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq) (12.5.82)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Installing collected packages: sacrebleu, pythainlp, fairseq\n",
            "  Attempting uninstall: sacrebleu\n",
            "    Found existing installation: sacrebleu 1.2.10\n",
            "    Uninstalling sacrebleu-1.2.10:\n",
            "      Successfully uninstalled sacrebleu-1.2.10\n",
            "  Attempting uninstall: pythainlp\n",
            "    Found existing installation: pythainlp 2.1.4\n",
            "    Uninstalling pythainlp-2.1.4:\n",
            "      Successfully uninstalled pythainlp-2.1.4\n",
            "  Attempting uninstall: fairseq\n",
            "    Found existing installation: fairseq 0.9.0\n",
            "    Uninstalling fairseq-0.9.0:\n",
            "      Successfully uninstalled fairseq-0.9.0\n",
            "Successfully installed fairseq-0.12.2 pythainlp-5.0.4 sacrebleu-2.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fairseq",
                  "pythainlp"
                ]
              },
              "id": "c379883498bb4925a5fcaeaae873bea0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky9f_tb12zLy"
      },
      "source": [
        "import time, os\n",
        "import html\n",
        "from functools import partial\n",
        "from collections import defaultdict\n",
        "\n",
        "from fairseq.models.transformer import TransformerModel\n",
        "\n",
        "from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
        "from pythainlp.tokenize import word_tokenize as th_word_tokenize\n",
        "\n",
        "en_word_tokenize = MosesTokenizer('en')\n",
        "en_word_detokenize = MosesDetokenizer('en')\n",
        "\n",
        "th_word_tokenize = partial(th_word_tokenize, keep_whitespace=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QQienYe2zL0"
      },
      "source": [
        "## Download model checkpoint, vocabulary and SentencePiece model.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0pY0tiv2zL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "2b8a64d2-1e15-457d-ce02-4f2394300ce5"
      },
      "source": [
        "!wget https://github.com/vistec-AI/model-releases/releases/download/SCB_1M%2BTBASE_v1.0/SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0.tar.gz\n",
        "!wget https://github.com/vistec-AI/model-releases/releases/download/SCB_1M%2BTBASE_v1.0/SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0.tar.gz\n",
        "!wget https://github.com/vistec-AI/model-releases/releases/download/SCB_1M%2BTBASE_v1.0/SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0.tar.gz\n",
        "!wget https://github.com/vistec-AI/model-releases/releases/download/SCB_1M%2BTBASE_v1.0/SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0.tar.gz"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-02 19:44:35--  https://github.com/vistec-AI/model-releases/releases/download/SCB_1M%2BTBASE_v1.0/SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/272403533/4c46a580-b4b5-11ea-8be6-db19f4a19e73?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240702T194435Z&X-Amz-Expires=300&X-Amz-Signature=2b2ffdf08b71db9c4e8117a75cc6ed201a6cee7c284e5d244ed769c9e0d5f932&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=272403533&response-content-disposition=attachment%3B%20filename%3DSCB_1M%2BTBASE_th-en_newmm-moses_130000-130000_v1.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-07-02 19:44:35--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/272403533/4c46a580-b4b5-11ea-8be6-db19f4a19e73?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240702T194435Z&X-Amz-Expires=300&X-Amz-Signature=2b2ffdf08b71db9c4e8117a75cc6ed201a6cee7c284e5d244ed769c9e0d5f932&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=272403533&response-content-disposition=attachment%3B%20filename%3DSCB_1M%2BTBASE_th-en_newmm-moses_130000-130000_v1.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1879697467 (1.8G) [application/octet-stream]\n",
            "Saving to: ‘SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0.tar.gz.2’\n",
            "\n",
            "SCB_1M+TBASE_th-en_ 100%[===================>]   1.75G   109MB/s    in 13s     \n",
            "\n",
            "2024-07-02 19:44:49 (136 MB/s) - ‘SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0.tar.gz.2’ saved [1879697467/1879697467]\n",
            "\n",
            "--2024-07-02 19:44:49--  https://github.com/vistec-AI/model-releases/releases/download/SCB_1M%2BTBASE_v1.0/SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/272403533/14416180-b4b9-11ea-81ab-f85e212bf35b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240702T194449Z&X-Amz-Expires=300&X-Amz-Signature=eaba0694492bff8af4f224a15c9ecd18e69b8f3ef70c190facc0dcc7431f0bd3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=272403533&response-content-disposition=attachment%3B%20filename%3DSCB_1M%2BTBASE_th-en_spm-spm_32000-joined_v1.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-07-02 19:44:49--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/272403533/14416180-b4b9-11ea-81ab-f85e212bf35b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240702T194449Z&X-Amz-Expires=300&X-Amz-Signature=eaba0694492bff8af4f224a15c9ecd18e69b8f3ef70c190facc0dcc7431f0bd3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=272403533&response-content-disposition=attachment%3B%20filename%3DSCB_1M%2BTBASE_th-en_spm-spm_32000-joined_v1.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 699200099 (667M) [application/octet-stream]\n",
            "Saving to: ‘SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0.tar.gz.2’\n",
            "\n",
            "SCB_1M+TBASE_th-en_ 100%[===================>] 666.81M   128MB/s    in 5.1s    \n",
            "\n",
            "2024-07-02 19:44:54 (130 MB/s) - ‘SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0.tar.gz.2’ saved [699200099/699200099]\n",
            "\n",
            "--2024-07-02 19:44:54--  https://github.com/vistec-AI/model-releases/releases/download/SCB_1M%2BTBASE_v1.0/SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/272403533/012b9300-b4b3-11ea-9a62-0ce38364f73c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240702T194455Z&X-Amz-Expires=300&X-Amz-Signature=9109269ca62d8a534013d2adb0519bd90f57901265bb147c12d9fd7346c6cbc6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=272403533&response-content-disposition=attachment%3B%20filename%3DSCB_1M%2BTBASE_en-th_moses-spm_130000-16000_v1.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-07-02 19:44:55--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/272403533/012b9300-b4b3-11ea-9a62-0ce38364f73c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240702T194455Z&X-Amz-Expires=300&X-Amz-Signature=9109269ca62d8a534013d2adb0519bd90f57901265bb147c12d9fd7346c6cbc6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=272403533&response-content-disposition=attachment%3B%20filename%3DSCB_1M%2BTBASE_en-th_moses-spm_130000-16000_v1.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1173591382 (1.1G) [application/octet-stream]\n",
            "Saving to: ‘SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0.tar.gz.2’\n",
            "\n",
            "SCB_1M+TBASE_en-th_ 100%[===================>]   1.09G   218MB/s    in 6.0s    \n",
            "\n",
            "2024-07-02 19:45:01 (186 MB/s) - ‘SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0.tar.gz.2’ saved [1173591382/1173591382]\n",
            "\n",
            "--2024-07-02 19:45:01--  https://github.com/vistec-AI/model-releases/releases/download/SCB_1M%2BTBASE_v1.0/SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/272403533/c88cb900-b4b4-11ea-9b7f-294881c2ca03?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240702T194501Z&X-Amz-Expires=300&X-Amz-Signature=5055221310ee55df6cf0025f21dbf5d580d14b3faf1f59aaf327555f73f4944c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=272403533&response-content-disposition=attachment%3B%20filename%3DSCB_1M%2BTBASE_en-th_spm-spm_32000-joined_v1.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-07-02 19:45:01--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/272403533/c88cb900-b4b4-11ea-9b7f-294881c2ca03?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240702T194501Z&X-Amz-Expires=300&X-Amz-Signature=5055221310ee55df6cf0025f21dbf5d580d14b3faf1f59aaf327555f73f4944c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=272403533&response-content-disposition=attachment%3B%20filename%3DSCB_1M%2BTBASE_en-th_spm-spm_32000-joined_v1.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 699080820 (667M) [application/octet-stream]\n",
            "Saving to: ‘SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0.tar.gz.2’\n",
            "\n",
            "SCB_1M+TBASE_en-th_ 100%[===================>] 666.70M   244MB/s    in 2.7s    \n",
            "\n",
            "2024-07-02 19:45:04 (244 MB/s) - ‘SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0.tar.gz.2’ saved [699080820/699080820]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--ALpegmc-Rx"
      },
      "source": [
        "!mkdir -p ./mt"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B40WUuUZLbML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "153cb125-0669-4639-f8b5-8e7a69f1895f"
      },
      "source": [
        "!head ./mt/SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0/vocab/dict.th.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ที่ 499637\n",
            "ของ 365072\n",
            "และ 342731\n",
            "ใน 297840\n",
            "การ 283022\n",
            "มี 273923\n",
            "ได้ 255623\n",
            ": 230075\n",
            "ไม่ 223501\n",
            "ให้ 203990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNveTJc3LsTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c1dbb77-b54d-4e96-9b53-82eca1e01b5f"
      },
      "source": [
        "!wc -l ./mt/SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0/vocab/dict.th.txt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129996 ./mt/SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0/vocab/dict.th.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5_wT0hULwzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5420aa70-308b-4854-d6e9-5f452b1cf9df"
      },
      "source": [
        "!wc -l ./mt/SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0/vocab/dict.en.txt"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129996 ./mt/SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0/vocab/dict.en.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hzWQgt_2zL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a2046a33-7041-408d-b6c9-c7a9b5fc53cb"
      },
      "source": [
        "!tar -C ./mt -xvzf SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0.tar.gz\n",
        "!tar -C ./mt -xvzf SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0.tar.gz\n",
        "!tar -C ./mt -xvzf SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0.tar.gz\n",
        "!tar -C ./mt -xvzf SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0.tar.gz"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0/\n",
            "SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0/models/\n",
            "SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0/vocab/\n",
            "SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0/vocab/dict.th.txt\n",
            "SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0/vocab/dict.en.txt\n",
            "SCB_1M+TBASE_th-en_newmm-moses_130000-130000_v1.0/models/checkpoint.pt\n",
            "SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0/\n",
            "SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0/bpe/\n",
            "SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0/models/\n",
            "SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0/vocab/\n",
            "SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0/vocab/dict.th.txt\n",
            "SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0/vocab/dict.en.txt\n",
            "SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0/models/checkpoint.pt\n",
            "SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0/bpe/spm.th.vocab\n",
            "SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0/bpe/spm.en.model\n",
            "SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0/bpe/spm.th.model\n",
            "SCB_1M+TBASE_th-en_spm-spm_32000-joined_v1.0/bpe/spm.en.vocab\n",
            "SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0/\n",
            "SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0/bpe/\n",
            "SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0/models/\n",
            "SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0/vocab/\n",
            "SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0/vocab/dict.th.txt\n",
            "SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0/vocab/dict.en.txt\n",
            "SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0/models/checkpoint.pt\n",
            "SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0/bpe/spm.th.vocab\n",
            "SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0/bpe/spm.th.model\n",
            "SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0/\n",
            "SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0/bpe/\n",
            "SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0/models/\n",
            "SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0/vocab/\n",
            "SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0/vocab/dict.th.txt\n",
            "SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0/vocab/dict.en.txt\n",
            "SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0/models/checkpoint.pt\n",
            "SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0/bpe/spm.th.vocab\n",
            "SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0/bpe/spm.en.model\n",
            "SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0/bpe/spm.th.model\n",
            "SCB_1M+TBASE_en-th_spm-spm_32000-joined_v1.0/bpe/spm.en.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With langchain"
      ],
      "metadata": {
        "id": "oOhylP1SxGQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community\n",
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LDGDHxV_wi5E",
        "outputId": "bb6fcee9-51cf-4f0a-9826-30ac0e20a179"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.6 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.6)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.10)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.83)\n",
            "Collecting numpy<2,>=1 (from langchain_community)\n",
            "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (2.7.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (2.18.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.0\n",
            "    Uninstalling numpy-2.0.0:\n",
            "      Successfully uninstalled numpy-2.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.83)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9qPrqetwU-V",
        "outputId": "e0feca2f-69d8-40cd-ed93-d853788174ad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "collapsed": true,
        "id": "m1o6QlLKwVVP",
        "outputId": "489ed8cf-df45-4035-f922-4a9ab5082120"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ee76963e-c0fa-44ea-a16c-4c5d8a7c9af8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ee76963e-c0fa-44ea-a16c-4c5d8a7c9af8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2304.05295v1.pdf to 2304.05295v1 (2).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Replace 'your_file.pdf' with the actual filename\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Open the PDF file\n",
        "with open(file_name, 'rb') as file:\n",
        "    reader = PyPDF2.PdfReader(file) # Use PdfReader instead of PdfFileReader\n",
        "    text = ''\n",
        "    for page_num in range(len(reader.pages)): # Use reader.pages to get the pages\n",
        "        page = reader.pages[page_num]\n",
        "        text += page.extract_text()"
      ],
      "metadata": {
        "id": "x-yC5ErIyGoH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.schema import Document\n",
        "import os\n",
        "\n",
        "# Assuming 'text' is your string variable containing the text data\n",
        "# Create a temporary file and write the text to it\n",
        "with open('temp.txt', 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "# Create a LangChain document from the temporary file\n",
        "documents = TextLoader('temp.txt').load()\n",
        "\n",
        "os.remove('temp.txt')"
      ],
      "metadata": {
        "id": "tZZrfNfEyJlv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1Ana-8yOyiJs",
        "outputId": "5c19185e-0916-4b38-95fe-95715ca8aea7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"A Comprehensive Study on Object Detection Techniques in \\nUnconstrained Environments  \\n \\nHrishitva Patel1 \\n1The University of Texas at San Antonio   \\nhrishitva.patel@my.utsa.edu   \\n \\nAbstract  \\nObject detection is a crucial task in computer vision that aims to identify and localize objects in \\nimages or videos. The recent advancements in deep learning and Convolutional Neural Networks \\n(CNNs) have significantly improved the performance of object detection techniques. This paper \\npresents a comprehensive study of object detection techniques in unconstrained environments, \\nincluding various challeng es, datasets, and state -of-the-art approaches. Additionally, we present a \\ncomparative analysis of the methods and highlight their strengths and weaknesses. Finally, we \\nprovide some future research directions to further improve object detection in unconstra ined \\nenvironments.  \\nKeywords:  object detection, unconstrained environments, deep learning, convolutional neural \\nnetworks, computer vision  \\n1 Introduction  and Background  \\nObject detection is a fundamental problem in computer vision, with numerous applications \\nspanning fields such as surveillance, robotics, autonomous vehicles, augmented reality, and \\nhuman -computer interaction. The primary goal of object detection is to recognize and localize \\ninstances of objects belonging to predefined classes in images or vide os. In recent years, significant \\nprogress has been made in the development of object detection algorithms, mainly due to the \\nemergence of deep learning and Convolutional Neural Networks (CNNs). These advancements \\nhave led to impressive performance improvem ents in various benchmark datasets, such as \\nPASCAL VOC, ImageNet, and MS COCO. Despite these successes, object detection in \\nunconstrained environments remains a challenging task. Unconstrained environments are \\ncharacterized by variations in lighting condit ions, viewpoint changes, occlusions, object \\ndeformations, scale changes, and the presence of cluttered backgrounds. These factors can severely \\naffect the performance of object detection algorithms, making it difficult to achieve high detection \\naccuracy and  robustness.   \\nIn recent years, significant progress has been made in object detection, particularly in the area of \\ndeep learning and Convolutional Neural Networks (CNNs)  [1]. These techniques have \\nsignificantly improved the performance of object detection algorithms, particularly in \\nunconstrained environments where objects may appear at different scales, angles, and orientations.  \\nRegion -based object detectors, such as Region -based Convolutional Neural Networks (R -CNN) \\n[2], operate by first generating region proposals using a selective search algorithm, which generates around 2000 regions per image. Each region is then passed through a CNN to generate \\na fixed -length feature vector, which is fed into a support vector machine (SVM)  [3] to classify the \\nregion and predict its bounding box coordinates. Finally, non -maximum suppression is applied to \\neliminate redundant detections. While R -CNN was a significant breakthrough in object detection, \\nit has several limitations, such as slow training and inference times.  \\n \\nTo address these issues, researchers have proposed several variants of R -CNN, such as Fast R -\\nCNN [4], which shares convolutional features across region proposals, and Faster R -CNN  , which \\nintroduces a Region Proposal Network (RPN) to generate region proposals in an end -to-end \\nmanner. These variants significantly improve the speed and accuracy of R -CNN, making it a \\npopular choice for  object detection in unconstrained environments.  The purpose of this paper is to \\nprovide a comprehensive overview of object detection techniques in unconstrained environments, \\naddressing the challenges, datasets, and state -of-the-art approaches.  \\nThe paper  is organized as follows: Section 2 discusses the challenges encountered in object \\ndetection in unconstrained environments, highlighting the factors that contribute to the complexity \\nof the problem. Section 3 presents a review of the commonly used datasets  for evaluating object \\ndetection techniques in unconstrained environments. Section 4 presents state of the art Objection \\ndetection techniques. Section 5 presents a comparative analysis of the surveyed methods, \\nemphasizing their strengths and weaknesses in terms of accuracy, computational complexity, and \\nrobustness to variations in the unconstrained environment. Section 6 concludes the paper by \\nhighlighting some of the open research questions and future directions in the field of object \\ndetection in unconstr ained environments.  \\n2 Challenges in Object Detection in Unconstrained Environments  \\n2.1 Illumination Changes  \\nVariations in lighting conditions, such as shadows and overexposure, can significantly impact the \\nappearance of objects, making it difficult for de tection algorithms to identify and localize them \\naccurately. Variations in lighting conditions, such as shadows, overexposure, or underexposure, \\ncan significantly impact the appearance of objects in images [5]. These changes can make it \\ndifficult for detection algorithms to identify and localize objects accurately. To address this issue, \\nseveral approaches have been proposed, including color constancy techniques [6] and deep \\nlearning -based methods that can learn illumination invariant features  [7].  \\n2.2 Viewpoint  Variation  \\nChanges in the viewpoint or camera angle can alter the object's appearance, causing the detection \\nalgorithm to fail in recognizing the object or produce inaccurate bounding boxes [8]. Several \\nmethods have been proposed to tackle this issue, such as viewpoint invariant features and multi -\\nview object detectors  [8]. 2.3 Occlusion  \\nObjects in the scene may be partially or entirely occluded by other objects, making it challenging \\nfor the detection algorithm to identify and localize t hem correctly [9]. To address occlusion, some \\nmethods employ part -based models [10] or leverage context information from surrounding regions.  \\n3 Datasets  \\nObject detection is a vital task in computer vision that involves identifying the presence and \\nlocation of objects in an image or video. To evaluate the performance of object detection \\ntechniques in unconstrained environments, several benchmark datasets have been  created. These \\ndatasets provide a standardized set of images with labeled objects, enabling researchers to compare \\nthe accuracy and speed of different algorithms. Some popular datasets include:  \\n3.1 Pascal V OC  \\nThe PASCAL VOC (Visual Object Classes) dataset is one of the oldest and most popular datasets \\nfor object detection. It contains 17,125 images with 20 object classes, such as person, car, and dog. \\nThe dataset provides bounding box annotations for each object in the image. PASCAL VOC has  \\nbeen used as a benchmark dataset for several years, and many state -of-the-art object detection \\ntechniques have been evaluated on this dataset.  \\n3.2 ImageNet  \\nThe ImageNet dataset is a massive dataset that contains 1.2 million images with 1,000 object \\nclasses. Unlike PASCAL VOC, ImageNet does not provide annotations for object detection. \\nHowever, many researchers have used this dataset to pre -train their models on a large amount of \\ndata before fine -tuning them on smaller object detection datasets.  \\n3.3 C OCO  \\nThe COCO (Common Objects in Context) dataset is a newer dataset that contains 330,000 images \\nwith 80 object classes. COCO provides more detailed annotations than PASCAL VOC, including \\nsegmentation masks for each object in the image. This makes COCO a more challenging dataset \\nfor object detection algorithms to perform well on.  \\n3.4 Open Images  \\nThe Open Images dataset is another large -scale dataset that contains 1.7 million images with 600 \\nobject classes. It provides both bounding box and segmentation mas k annotations and  has been \\nused as a benchmark for object detection algorithms that require large amounts of training data.  \\nThese datasets vary in size, number of classes, and annotation types, allowing researchers to test \\ntheir algorithms on a wide range of scenarios. The following table summarizes some key \\ninformation about the four popular benchmark datasets used for evaluating object detection \\ntechniques:  Table 1. Summary of key information about benchmark datasets for object detection  \\nDataset Name  Number of Images  Number of Classes  Annotation Type  \\nPASCAL VOC [11] 17,125  20 Bounding Boxes  \\nImageNet  [12] 1.2 million  1,000  Bounding Boxes  \\nCOCO  [13] 330,000  80 Bounding Boxes  \\nOpen Images  [14] 1.7 million  600 Mask RCNN  \\n \\n4 State -of-the-art Object Detection Techniques  \\nWe categorize the state -of-the-art object detection techniques into two main groups: two -stage \\ndetectors and single -stage detectors.  \\n \\nFigure 1. Milestones of object detection  [15]. \\n4.1 Two -stage detectors  \\nTwo-stage detectors consist of a region proposal stage followed by a classification stage. Some \\nprominent two -stage detectors include:  \\n4.1.1 R-CNN  \\nR-CNN (Region -based Convolutional Neural Networks) is an object detection model that was \\nproposed in 2014 by Ross Girshick et al. R -CNN is a two -stage object detection framework that \\nuses a region proposal mechanism to generate potential object regio ns in an image and then applies \\na convolutional neural network (CNN) to classify and refine these regions.  \\nThe R -CNN framework consists of the following steps:  \\n1. Region Proposal: The first stage of R -CNN generates potential object regions by using a \\nselecti ve search algorithm that combines low -level features, such as color and texture, with \\nhigh-level cues, such as edges and corners. Selective search generates around 2,000 region \\nproposals for each image.  \\n2. Feature Extraction: In the second stage, each region proposal is warped to a fixed size and \\nfed through a pre -trained CNN, such as Alex Net  or VGG, to extract a feature vector for \\nthat region.  \\n3. Object Classification and Refinement: The feature vector for each region proposal is then \\nfed into a set of fully co nnected layers that perform object classification and bounding box \\nregression. The classification layer outputs the probability of each region proposal \\ncontaining a particular object class, while the regression layer outputs the refined bounding \\nbox coordi nates for that object class.  \\n4.1.2 Fast R -CNN  \\nFaster R -CNN (Region -based Convolutional Neural Networks): Faster R -CNN is a two -stage \\nobject detection model that uses a Region Proposal Network (RPN) to generate object proposals \\nand a Fast R -CNN network to c lassify and refine the proposals. The RPN generates region \\nproposals by sliding a small network over the convolutional feature map and predicting abjectness  \\nscores and bounding box offsets. Faster R -CNN is known for its accuracy and has been widely \\nused in  object detection tasks.  \\n4.2 Single -stage detectors  \\nSingle -stage detectors directly predict object bounding boxes and class probabilities from an image. \\nSome popular single -stage detectors include:  \\n4.2.1 YOLO  \\nYOLO is another one -stage object detection model that predicts object class scores and bounding \\nbox offsets directly from the entire image. YOLO divides the image into a grid of cells and predicts \\nthe class and bounding box for each cell. YOLO uses a single neural network to make predicti ons \\nand is known for its speed and real -time performance.  \\n4.2.2 SSD  \\nThe Single Shot MultiBox Detector (SSD) extends the concept of YOLO by predicting bounding \\nboxes and class probabilities at multiple scales, which improves the detection of objects with \\nvarying sizes.  SSD uses a feature extractor to generate convolutional feature maps and applies a \\nset of convolutional filters to predict class scores and offsets for each default box. SSD is known \\nfor its speed and efficiency and has been used in real-time object detection applications.  4.2.3 Retina Net  \\nRetina Net  introduces the Focal Loss, which addresses the issue of class imbalance by down \\nweighting  the contribution of easy examples and focusing on hard examples during training. This \\nresults in improved detection performance, particularly for small objects.  Retina Net uses a novel \\nfocal loss function that assigns higher weights to hard examples and reduces the effect of easy \\nexamples during training. Retina Net also uses a Feature Pyramid Network  (FPN) to handle objects \\nat different scales and has achieved state -of-the-art performance on several object detection \\nbenchmarks.  \\n \\nFigure 2. One stage vs two stage object detection .  \\nThe below  table summarizes some key features of these state -of-the-art object detection techniques:  \\nTable 2. Key features of state -of-the-art object detection techniques  \\nTechnique  Training Time  Inference Time  Number of Parameters  AP on COCO  \\nFaster R -CNN  [16] Long  Medium  High  39.3 \\nSSD [17] Medium  Fast Low 31.2 \\nYOLO  [18] Short  Very Fast  Low 28.2 \\nRetina Net  [19] Long  Medium  High  39.1 \\n \\n5 Comparative Analysis  \\nIn this section, we compare the performance of various object detection techniques on the COCO \\ndataset [5]. The results are summarized in Table 1.  \\nTable 1: Comparison of object detection techniques on the COCO dataset  \\nMethod  Average Precision (AP)  Speed (fps)  \\nR-CNN 53.3 0.5 \\nFast R -CNN  70.0 5 \\nFaster R -CNN  73.2 7 \\nYOLOv3  57.9 45 \\nSSD 72.1 19 \\nRetinaNet  74.8 12 \\n \\nThe results in Table 1 show that two -stage detectors, such as Faster R -CNN, generally achieve \\nhigher average precision (AP) compared to single -stage detectors like YOLOv3 and SSD. \\nHowever, single -stage detectors are faster in terms of frames per second (fps), making them more \\nsuitable for real -time applications.  \\n \\nFigure 3. Comparison of object detection techniques on the COCO dataset  \\n 6 Conclusion and Future Directions  \\nIn this paper, we have presented a comprehensive study on object detection techniques in \\nunconstrained environments. We have discussed the challenges associated with object detection \\nin suc h environments, presented popular datasets, and provided an overview of the state -of-the-art \\ntechniques. Additionally, we have compared the performance of various methods and highlighted \\ntheir strengths and weaknesses.  \\nDespite the significant progress made  in recent years, object detection in unconstrained \\nenvironments remains a challenging problem. Future research directions could focus on the \\nfollowing aspects:  \\n• Developing more robust algorithms capable of handling occlusions, lighting variations, and \\nbackground clutter.  \\n• Investigating techniques for efficient and accurate detection of small -scale objects.  \\n• Exploring the integration of other sensor modalities, such as LiDAR or depth information, \\nto enhance object detection performance.  \\n• Developing unsupervised or weakly supervised object detection techniques to reduce the \\nreliance on large -scale annotated datasets.  \\nBy addressing these challenges and exploring new approaches, we believe that object detection in \\nunconstrained environments can be further improved, paving the way for more reliable and \\nefficient applications in various domains, such as autonomous vehicles, robotics, and surveillance \\nsystems.  \\nReferences  \\n \\n1. Dhillon, A. and G.K.J.P.i.A.I. Verma, Convolutional neural network: a review of models, \\nmethodologies and applications to object detection. 2020. 9(2): p. 85 -112. \\n2. Xu, C., et al., A page object detection method based on mask R -CNN. 2021. 9: p. 143448 -\\n143457. \\n3. Qin, W., R. Elanwar, and M.J.J.o.I.S. Betke, Text and metadata extraction from scanned \\nArabic documents using support vector machines. 2022. 48(2): p. 268 -279. \\n4. Liu, B., W. Zhao, and Q. Sun. Study of object detection based on Faster R -CNN. in 201 7 \\nChinese Automation Congress (CAC). 2017. IEEE.  \\n5. Wong, J.K.W., et al., Development of a refined illumination and reflectance approach for \\noptimal construction site interior image enhancement. 2022.  \\n6. Li, Y., et al., A unified probabilistic framework of  robust and efficient color consistency \\ncorrection for multiple images. 2022. 190: p. 1 -24. \\n7. Csurka, G. and M.J.a.p.a. Humenberger, From handcrafted to deep local invariant features. \\n2018. 2: p. 1.  \\n8. Doi, K., et al., Detecting Object -Level Scene Changes  in Images with Viewpoint \\nDifferences Using Graph Matching. 2022. 14(17): p. 4225.  \\n9. Cao, Z., et al., A Multi -Object Tracking Algorithm With Center -Based Feature Extraction \\nand Occlusion Handling. 2022.  \\n10. Somers, V., C. De Vleeschouwer, and A. Alahi. Bo dy Part -Based Representation Learning \\nfor Occluded Person Re -Identification. in Proceedings of the IEEE/CVF Winter \\nConference on Applications of Computer Vision. 2023.  \\n11. Hwang, B., S. Lee, and H.J.E. Han, LNFCOS: Efficient Object Detection through Deep \\nLearning Based on LNblock. 2022. 11(17): p. 2783.  12. Zhao, J., et al., Data -adaptive binary neural networks for efficient object detection and \\nrecognition. 2022. 153: p. 239 -245. \\n13. Ma, J., Y. Ushiku, and M. Sagara. The effect of improving annotation qual ity on object \\ndetection datasets: A preliminary study. in Proceedings of the IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition. 2022.  \\n14. Kuznetsova, A., et al., The open images dataset v4: Unified image classification, object \\ndetection, and v isual relationship detection at scale. 2020. 128(7): p. 1956 -1981.  \\n15. Xiao, Y., et al., A review of object detection based on deep learning. 2020. 79: p. 23729 -\\n23791.  \\n16. Rani, S., et al., Object detection and recognition using contour based edge detectio n and \\nfast R -CNN. 2022. 81(29): p. 42183 -42207.  \\n17. Feroz, M.A., et al. Object detection and classification from a real -time video using SSD \\nand YOLO models. in Computational Intelligence in Pattern Recognition: Proceedings of \\nCIPR 2021. 2022. Springer.  \\n18. Diwan, T., et al., Object detection using YOLO: Challenges, architectural successors, \\ndatasets and applications. 2022: p. 1 -33. \\n19. Li, G., et al. Knowledge distillation for object detection via rank mimicking and prediction -\\nguided feature imitation. i n Proceedings of the AAAI Conference on Artificial Intelligence. \\n2022.  \\n \", metadata={'source': 'temp.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
        "import re"
      ],
      "metadata": {
        "id": "DXKMcnsGzAuN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "wu1yEigpzVA6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azZaxtqqzhHB",
        "outputId": "d26a1f8b-331c-4759-ab21-7581670b1e25"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='A Comprehensive Study on Object Detection Techniques in \\nUnconstrained Environments  \\n \\nHrishitva Patel1 \\n1The University of Texas at San Antonio   \\nhrishitva.patel@my.utsa.edu   \\n \\nAbstract  \\nObject detection is a crucial task in computer vision that aims to identify and localize objects in \\nimages or videos. The recent advancements in deep learning and Convolutional Neural Networks \\n(CNNs) have significantly improved the performance of object detection techniques. This paper \\npresents a comprehensive study of object detection techniques in unconstrained environments, \\nincluding various challeng es, datasets, and state -of-the-art approaches. Additionally, we present a \\ncomparative analysis of the methods and highlight their strengths and weaknesses. Finally, we \\nprovide some future research directions to further improve object detection in unconstra ined \\nenvironments.  \\nKeywords:  object detection, unconstrained environments, deep learning, convolutional neural \\nnetworks, computer vision  \\n1 Introduction  and Background  \\nObject detection is a fundamental problem in computer vision, with numerous applications \\nspanning fields such as surveillance, robotics, autonomous vehicles, augmented reality, and \\nhuman -computer interaction. The primary goal of object detection is to recognize and localize \\ninstances of objects belonging to predefined classes in images or vide os. In recent years, significant \\nprogress has been made in the development of object detection algorithms, mainly due to the \\nemergence of deep learning and Convolutional Neural Networks (CNNs). These advancements \\nhave led to impressive performance improvem ents in various benchmark datasets, such as \\nPASCAL VOC, ImageNet, and MS COCO. Despite these successes, object detection in \\nunconstrained environments remains a challenging task. Unconstrained environments are \\ncharacterized by variations in lighting condit ions, viewpoint changes, occlusions, object', metadata={'source': 'temp.txt'})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Translate each chunk\n",
        "# translated_chunks = [translate_and_clean(chunk) for chunk in sentence_chunks]\n",
        "\n",
        "# # Combine the translated chunks\n",
        "# final_translated_text = ' '.join(translated_chunks)"
      ],
      "metadata": {
        "id": "UkmEL3h0DdbU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Original Text:\\n\", text)\n",
        "# print(\"\\nTranslated Text:\\n\", final_translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qIY_wTsvDdYl",
        "outputId": "7aa4c996-a71b-4dbb-9f16-4ec72202f343"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " A Comprehensive Study on Object Detection Techniques in \n",
            "Unconstrained Environments  \n",
            " \n",
            "Hrishitva Patel1 \n",
            "1The University of Texas at San Antonio   \n",
            "hrishitva.patel@my.utsa.edu   \n",
            " \n",
            "Abstract  \n",
            "Object detection is a crucial task in computer vision that aims to identify and localize objects in \n",
            "images or videos. The recent advancements in deep learning and Convolutional Neural Networks \n",
            "(CNNs) have significantly improved the performance of object detection techniques. This paper \n",
            "presents a comprehensive study of object detection techniques in unconstrained environments, \n",
            "including various challeng es, datasets, and state -of-the-art approaches. Additionally, we present a \n",
            "comparative analysis of the methods and highlight their strengths and weaknesses. Finally, we \n",
            "provide some future research directions to further improve object detection in unconstra ined \n",
            "environments.  \n",
            "Keywords:  object detection, unconstrained environments, deep learning, convolutional neural \n",
            "networks, computer vision  \n",
            "1 Introduction  and Background  \n",
            "Object detection is a fundamental problem in computer vision, with numerous applications \n",
            "spanning fields such as surveillance, robotics, autonomous vehicles, augmented reality, and \n",
            "human -computer interaction. The primary goal of object detection is to recognize and localize \n",
            "instances of objects belonging to predefined classes in images or vide os. In recent years, significant \n",
            "progress has been made in the development of object detection algorithms, mainly due to the \n",
            "emergence of deep learning and Convolutional Neural Networks (CNNs). These advancements \n",
            "have led to impressive performance improvem ents in various benchmark datasets, such as \n",
            "PASCAL VOC, ImageNet, and MS COCO. Despite these successes, object detection in \n",
            "unconstrained environments remains a challenging task. Unconstrained environments are \n",
            "characterized by variations in lighting condit ions, viewpoint changes, occlusions, object \n",
            "deformations, scale changes, and the presence of cluttered backgrounds. These factors can severely \n",
            "affect the performance of object detection algorithms, making it difficult to achieve high detection \n",
            "accuracy and  robustness.   \n",
            "In recent years, significant progress has been made in object detection, particularly in the area of \n",
            "deep learning and Convolutional Neural Networks (CNNs)  [1]. These techniques have \n",
            "significantly improved the performance of object detection algorithms, particularly in \n",
            "unconstrained environments where objects may appear at different scales, angles, and orientations.  \n",
            "Region -based object detectors, such as Region -based Convolutional Neural Networks (R -CNN) \n",
            "[2], operate by first generating region proposals using a selective search algorithm, which generates around 2000 regions per image. Each region is then passed through a CNN to generate \n",
            "a fixed -length feature vector, which is fed into a support vector machine (SVM)  [3] to classify the \n",
            "region and predict its bounding box coordinates. Finally, non -maximum suppression is applied to \n",
            "eliminate redundant detections. While R -CNN was a significant breakthrough in object detection, \n",
            "it has several limitations, such as slow training and inference times.  \n",
            " \n",
            "To address these issues, researchers have proposed several variants of R -CNN, such as Fast R -\n",
            "CNN [4], which shares convolutional features across region proposals, and Faster R -CNN  , which \n",
            "introduces a Region Proposal Network (RPN) to generate region proposals in an end -to-end \n",
            "manner. These variants significantly improve the speed and accuracy of R -CNN, making it a \n",
            "popular choice for  object detection in unconstrained environments.  The purpose of this paper is to \n",
            "provide a comprehensive overview of object detection techniques in unconstrained environments, \n",
            "addressing the challenges, datasets, and state -of-the-art approaches.  \n",
            "The paper  is organized as follows: Section 2 discusses the challenges encountered in object \n",
            "detection in unconstrained environments, highlighting the factors that contribute to the complexity \n",
            "of the problem. Section 3 presents a review of the commonly used datasets  for evaluating object \n",
            "detection techniques in unconstrained environments. Section 4 presents state of the art Objection \n",
            "detection techniques. Section 5 presents a comparative analysis of the surveyed methods, \n",
            "emphasizing their strengths and weaknesses in terms of accuracy, computational complexity, and \n",
            "robustness to variations in the unconstrained environment. Section 6 concludes the paper by \n",
            "highlighting some of the open research questions and future directions in the field of object \n",
            "detection in unconstr ained environments.  \n",
            "2 Challenges in Object Detection in Unconstrained Environments  \n",
            "2.1 Illumination Changes  \n",
            "Variations in lighting conditions, such as shadows and overexposure, can significantly impact the \n",
            "appearance of objects, making it difficult for de tection algorithms to identify and localize them \n",
            "accurately. Variations in lighting conditions, such as shadows, overexposure, or underexposure, \n",
            "can significantly impact the appearance of objects in images [5]. These changes can make it \n",
            "difficult for detection algorithms to identify and localize objects accurately. To address this issue, \n",
            "several approaches have been proposed, including color constancy techniques [6] and deep \n",
            "learning -based methods that can learn illumination invariant features  [7].  \n",
            "2.2 Viewpoint  Variation  \n",
            "Changes in the viewpoint or camera angle can alter the object's appearance, causing the detection \n",
            "algorithm to fail in recognizing the object or produce inaccurate bounding boxes [8]. Several \n",
            "methods have been proposed to tackle this issue, such as viewpoint invariant features and multi -\n",
            "view object detectors  [8]. 2.3 Occlusion  \n",
            "Objects in the scene may be partially or entirely occluded by other objects, making it challenging \n",
            "for the detection algorithm to identify and localize t hem correctly [9]. To address occlusion, some \n",
            "methods employ part -based models [10] or leverage context information from surrounding regions.  \n",
            "3 Datasets  \n",
            "Object detection is a vital task in computer vision that involves identifying the presence and \n",
            "location of objects in an image or video. To evaluate the performance of object detection \n",
            "techniques in unconstrained environments, several benchmark datasets have been  created. These \n",
            "datasets provide a standardized set of images with labeled objects, enabling researchers to compare \n",
            "the accuracy and speed of different algorithms. Some popular datasets include:  \n",
            "3.1 Pascal V OC  \n",
            "The PASCAL VOC (Visual Object Classes) dataset is one of the oldest and most popular datasets \n",
            "for object detection. It contains 17,125 images with 20 object classes, such as person, car, and dog. \n",
            "The dataset provides bounding box annotations for each object in the image. PASCAL VOC has  \n",
            "been used as a benchmark dataset for several years, and many state -of-the-art object detection \n",
            "techniques have been evaluated on this dataset.  \n",
            "3.2 ImageNet  \n",
            "The ImageNet dataset is a massive dataset that contains 1.2 million images with 1,000 object \n",
            "classes. Unlike PASCAL VOC, ImageNet does not provide annotations for object detection. \n",
            "However, many researchers have used this dataset to pre -train their models on a large amount of \n",
            "data before fine -tuning them on smaller object detection datasets.  \n",
            "3.3 C OCO  \n",
            "The COCO (Common Objects in Context) dataset is a newer dataset that contains 330,000 images \n",
            "with 80 object classes. COCO provides more detailed annotations than PASCAL VOC, including \n",
            "segmentation masks for each object in the image. This makes COCO a more challenging dataset \n",
            "for object detection algorithms to perform well on.  \n",
            "3.4 Open Images  \n",
            "The Open Images dataset is another large -scale dataset that contains 1.7 million images with 600 \n",
            "object classes. It provides both bounding box and segmentation mas k annotations and  has been \n",
            "used as a benchmark for object detection algorithms that require large amounts of training data.  \n",
            "These datasets vary in size, number of classes, and annotation types, allowing researchers to test \n",
            "their algorithms on a wide range of scenarios. The following table summarizes some key \n",
            "information about the four popular benchmark datasets used for evaluating object detection \n",
            "techniques:  Table 1. Summary of key information about benchmark datasets for object detection  \n",
            "Dataset Name  Number of Images  Number of Classes  Annotation Type  \n",
            "PASCAL VOC [11] 17,125  20 Bounding Boxes  \n",
            "ImageNet  [12] 1.2 million  1,000  Bounding Boxes  \n",
            "COCO  [13] 330,000  80 Bounding Boxes  \n",
            "Open Images  [14] 1.7 million  600 Mask RCNN  \n",
            " \n",
            "4 State -of-the-art Object Detection Techniques  \n",
            "We categorize the state -of-the-art object detection techniques into two main groups: two -stage \n",
            "detectors and single -stage detectors.  \n",
            " \n",
            "Figure 1. Milestones of object detection  [15]. \n",
            "4.1 Two -stage detectors  \n",
            "Two-stage detectors consist of a region proposal stage followed by a classification stage. Some \n",
            "prominent two -stage detectors include:  \n",
            "4.1.1 R-CNN  \n",
            "R-CNN (Region -based Convolutional Neural Networks) is an object detection model that was \n",
            "proposed in 2014 by Ross Girshick et al. R -CNN is a two -stage object detection framework that \n",
            "uses a region proposal mechanism to generate potential object regio ns in an image and then applies \n",
            "a convolutional neural network (CNN) to classify and refine these regions.  \n",
            "The R -CNN framework consists of the following steps:  \n",
            "1. Region Proposal: The first stage of R -CNN generates potential object regions by using a \n",
            "selecti ve search algorithm that combines low -level features, such as color and texture, with \n",
            "high-level cues, such as edges and corners. Selective search generates around 2,000 region \n",
            "proposals for each image.  \n",
            "2. Feature Extraction: In the second stage, each region proposal is warped to a fixed size and \n",
            "fed through a pre -trained CNN, such as Alex Net  or VGG, to extract a feature vector for \n",
            "that region.  \n",
            "3. Object Classification and Refinement: The feature vector for each region proposal is then \n",
            "fed into a set of fully co nnected layers that perform object classification and bounding box \n",
            "regression. The classification layer outputs the probability of each region proposal \n",
            "containing a particular object class, while the regression layer outputs the refined bounding \n",
            "box coordi nates for that object class.  \n",
            "4.1.2 Fast R -CNN  \n",
            "Faster R -CNN (Region -based Convolutional Neural Networks): Faster R -CNN is a two -stage \n",
            "object detection model that uses a Region Proposal Network (RPN) to generate object proposals \n",
            "and a Fast R -CNN network to c lassify and refine the proposals. The RPN generates region \n",
            "proposals by sliding a small network over the convolutional feature map and predicting abjectness  \n",
            "scores and bounding box offsets. Faster R -CNN is known for its accuracy and has been widely \n",
            "used in  object detection tasks.  \n",
            "4.2 Single -stage detectors  \n",
            "Single -stage detectors directly predict object bounding boxes and class probabilities from an image. \n",
            "Some popular single -stage detectors include:  \n",
            "4.2.1 YOLO  \n",
            "YOLO is another one -stage object detection model that predicts object class scores and bounding \n",
            "box offsets directly from the entire image. YOLO divides the image into a grid of cells and predicts \n",
            "the class and bounding box for each cell. YOLO uses a single neural network to make predicti ons \n",
            "and is known for its speed and real -time performance.  \n",
            "4.2.2 SSD  \n",
            "The Single Shot MultiBox Detector (SSD) extends the concept of YOLO by predicting bounding \n",
            "boxes and class probabilities at multiple scales, which improves the detection of objects with \n",
            "varying sizes.  SSD uses a feature extractor to generate convolutional feature maps and applies a \n",
            "set of convolutional filters to predict class scores and offsets for each default box. SSD is known \n",
            "for its speed and efficiency and has been used in real-time object detection applications.  4.2.3 Retina Net  \n",
            "Retina Net  introduces the Focal Loss, which addresses the issue of class imbalance by down \n",
            "weighting  the contribution of easy examples and focusing on hard examples during training. This \n",
            "results in improved detection performance, particularly for small objects.  Retina Net uses a novel \n",
            "focal loss function that assigns higher weights to hard examples and reduces the effect of easy \n",
            "examples during training. Retina Net also uses a Feature Pyramid Network  (FPN) to handle objects \n",
            "at different scales and has achieved state -of-the-art performance on several object detection \n",
            "benchmarks.  \n",
            " \n",
            "Figure 2. One stage vs two stage object detection .  \n",
            "The below  table summarizes some key features of these state -of-the-art object detection techniques:  \n",
            "Table 2. Key features of state -of-the-art object detection techniques  \n",
            "Technique  Training Time  Inference Time  Number of Parameters  AP on COCO  \n",
            "Faster R -CNN  [16] Long  Medium  High  39.3 \n",
            "SSD [17] Medium  Fast Low 31.2 \n",
            "YOLO  [18] Short  Very Fast  Low 28.2 \n",
            "Retina Net  [19] Long  Medium  High  39.1 \n",
            " \n",
            "5 Comparative Analysis  \n",
            "In this section, we compare the performance of various object detection techniques on the COCO \n",
            "dataset [5]. The results are summarized in Table 1.  \n",
            "Table 1: Comparison of object detection techniques on the COCO dataset  \n",
            "Method  Average Precision (AP)  Speed (fps)  \n",
            "R-CNN 53.3 0.5 \n",
            "Fast R -CNN  70.0 5 \n",
            "Faster R -CNN  73.2 7 \n",
            "YOLOv3  57.9 45 \n",
            "SSD 72.1 19 \n",
            "RetinaNet  74.8 12 \n",
            " \n",
            "The results in Table 1 show that two -stage detectors, such as Faster R -CNN, generally achieve \n",
            "higher average precision (AP) compared to single -stage detectors like YOLOv3 and SSD. \n",
            "However, single -stage detectors are faster in terms of frames per second (fps), making them more \n",
            "suitable for real -time applications.  \n",
            " \n",
            "Figure 3. Comparison of object detection techniques on the COCO dataset  \n",
            " 6 Conclusion and Future Directions  \n",
            "In this paper, we have presented a comprehensive study on object detection techniques in \n",
            "unconstrained environments. We have discussed the challenges associated with object detection \n",
            "in suc h environments, presented popular datasets, and provided an overview of the state -of-the-art \n",
            "techniques. Additionally, we have compared the performance of various methods and highlighted \n",
            "their strengths and weaknesses.  \n",
            "Despite the significant progress made  in recent years, object detection in unconstrained \n",
            "environments remains a challenging problem. Future research directions could focus on the \n",
            "following aspects:  \n",
            "• Developing more robust algorithms capable of handling occlusions, lighting variations, and \n",
            "background clutter.  \n",
            "• Investigating techniques for efficient and accurate detection of small -scale objects.  \n",
            "• Exploring the integration of other sensor modalities, such as LiDAR or depth information, \n",
            "to enhance object detection performance.  \n",
            "• Developing unsupervised or weakly supervised object detection techniques to reduce the \n",
            "reliance on large -scale annotated datasets.  \n",
            "By addressing these challenges and exploring new approaches, we believe that object detection in \n",
            "unconstrained environments can be further improved, paving the way for more reliable and \n",
            "efficient applications in various domains, such as autonomous vehicles, robotics, and surveillance \n",
            "systems.  \n",
            "References  \n",
            " \n",
            "1. Dhillon, A. and G.K.J.P.i.A.I. Verma, Convolutional neural network: a review of models, \n",
            "methodologies and applications to object detection. 2020. 9(2): p. 85 -112. \n",
            "2. Xu, C., et al., A page object detection method based on mask R -CNN. 2021. 9: p. 143448 -\n",
            "143457. \n",
            "3. Qin, W., R. Elanwar, and M.J.J.o.I.S. Betke, Text and metadata extraction from scanned \n",
            "Arabic documents using support vector machines. 2022. 48(2): p. 268 -279. \n",
            "4. Liu, B., W. Zhao, and Q. Sun. Study of object detection based on Faster R -CNN. in 201 7 \n",
            "Chinese Automation Congress (CAC). 2017. IEEE.  \n",
            "5. Wong, J.K.W., et al., Development of a refined illumination and reflectance approach for \n",
            "optimal construction site interior image enhancement. 2022.  \n",
            "6. Li, Y., et al., A unified probabilistic framework of  robust and efficient color consistency \n",
            "correction for multiple images. 2022. 190: p. 1 -24. \n",
            "7. Csurka, G. and M.J.a.p.a. Humenberger, From handcrafted to deep local invariant features. \n",
            "2018. 2: p. 1.  \n",
            "8. Doi, K., et al., Detecting Object -Level Scene Changes  in Images with Viewpoint \n",
            "Differences Using Graph Matching. 2022. 14(17): p. 4225.  \n",
            "9. Cao, Z., et al., A Multi -Object Tracking Algorithm With Center -Based Feature Extraction \n",
            "and Occlusion Handling. 2022.  \n",
            "10. Somers, V., C. De Vleeschouwer, and A. Alahi. Bo dy Part -Based Representation Learning \n",
            "for Occluded Person Re -Identification. in Proceedings of the IEEE/CVF Winter \n",
            "Conference on Applications of Computer Vision. 2023.  \n",
            "11. Hwang, B., S. Lee, and H.J.E. Han, LNFCOS: Efficient Object Detection through Deep \n",
            "Learning Based on LNblock. 2022. 11(17): p. 2783.  12. Zhao, J., et al., Data -adaptive binary neural networks for efficient object detection and \n",
            "recognition. 2022. 153: p. 239 -245. \n",
            "13. Ma, J., Y. Ushiku, and M. Sagara. The effect of improving annotation qual ity on object \n",
            "detection datasets: A preliminary study. in Proceedings of the IEEE/CVF Conference on \n",
            "Computer Vision and Pattern Recognition. 2022.  \n",
            "14. Kuznetsova, A., et al., The open images dataset v4: Unified image classification, object \n",
            "detection, and v isual relationship detection at scale. 2020. 128(7): p. 1956 -1981.  \n",
            "15. Xiao, Y., et al., A review of object detection based on deep learning. 2020. 79: p. 23729 -\n",
            "23791.  \n",
            "16. Rani, S., et al., Object detection and recognition using contour based edge detectio n and \n",
            "fast R -CNN. 2022. 81(29): p. 42183 -42207.  \n",
            "17. Feroz, M.A., et al. Object detection and classification from a real -time video using SSD \n",
            "and YOLO models. in Computational Intelligence in Pattern Recognition: Proceedings of \n",
            "CIPR 2021. 2022. Springer.  \n",
            "18. Diwan, T., et al., Object detection using YOLO: Challenges, architectural successors, \n",
            "datasets and applications. 2022: p. 1 -33. \n",
            "19. Li, G., et al. Knowledge distillation for object detection via rank mimicking and prediction -\n",
            "guided feature imitation. i n Proceedings of the AAAI Conference on Artificial Intelligence. \n",
            "2022.  \n",
            " \n",
            "\n",
            "Translated Text:\n",
            " การศึกษา อย่าง ครอบคลุม เกี่ยวกับ เทคนิค การ ตรวจจับ วัตถุ ในรูปแบบ การ ก   า หน ด พารา มิเตอร์   University  of  T ex f or ce   ของ เท็กซัส ที่ การตรวจสอบ วัตถุ ใน  S an  An ton io  An ton io   TM   ถือเป็น งาน สําคัญ ใน วิสัยทัศน์ ทาง คอมพิวเตอร์ ซึ่ง มุ่ง ที่จะ ระบุ และ ระบุ วัตถุ ต่าง  ๆ   ในการ ระบุ และ ระบุ ปัญหา ใน คอมพิวเตอร์ หรือ การ ตรวจจับ วัตถุ ที่เกิดขึ้น   ความก้าวหน้า ล่าสุด ใน การเรียนรู้ ลึก และ เครือข่าย  Ne ur al  In te mp ation  Network s   ได้ ปรับปรุง ประสิทธิภาพ ของ การตรวจสอบ วัตถุ   เครือข่าย  Ne ur al   TM   ได้ ปรับปรุง ประสิทธิภาพ ของ เทคนิค การ ตรวจจับ วัตถุ ใน  W e ff ect   TM  ซึ่ง รวมถึง เทคนิค การ ตรวจจับ วัตถุ หลาย  ๆ   อย่าง ใน  S n ow f or r ame s   ต่าง  ๆ   ที่ ได้ แสดง ผล การวิเคราะห์ เปรียบเทียบ ของ วิธีการ และ เน้น จุด แข็ง และ จุด โห ว ่ และ จุด โห ว ่ ของ วัตถุ ต่าง  ๆ  เรา นําเสนอ ทิศทาง การวิจัย ในอนาคต เพื่อ ปรับปรุง การ ตรวจจับ วัตถุ ในอนาคต ใน  S an  An ton io  An ton io   ได้ ปรับตัว ขึ้น จริง   มุมมอง ของ วัตถุ   ne ปัจจัย เหล่านี้ อาจ ส่งผลกระทบ อย่างรุนแรง ต่อ ประสิทธิภาพ ของการ ตรวจจับ วัตถุ   e . C N . com  ทําให้ ยากที่จะ บรรลุ ความแม่นยํา และ การตรวจสอบ สูง ใน ขั้นตอน ล่าสุด   ได้ มี ความคืบหน้า ที่สําคัญ ใน การจัด ประเภท วัตถุ โดย เฉพาะอย่างยิ่ง ในพื้นที่ ของ การเรียนรู้ ลึก และ เครือข่าย  Ne ur al  In tr ent  Network s   TM   เทคนิค เหล่านี้ ได้ ปรับปรุง ประสิทธิภาพ ของ การ ตรวจจับ วัตถุ อย่างมีนัยสําคัญ โดย เฉพาะอย่างยิ่ง ใน สภาพแวดล้อม   ex tr act   ที่ วัตถุ อาจ ปรากฏ ที่   ex tr uc t ur al  f e ature s   ภูมิภาค และ พื้นที่ อื่น  ๆ  -  C N N   ได้ ทํางาน โดย ข้อเสนอ การสร้าง ภูมิภาค โดย ข้อเสนอ การสร้าง ภูมิภาค โดย การเลือก ซึ่ง สร้าง พื้นที่ ประมาณ  2000   ภูมิภาค ต่อ  C N N   หนึ่ง แล้ว ผ่าน  C N N   เพื่อสร้าง คุณสมบัติ   e . C   ที่ คงที่ ซึ่ง ถูก ป้อน เข้าไปใน เครื่อง เว ก เตอร์ สนับสนุน การ แยก และ คาดการณ์ ขอบเขต ของ กล่อง   e . P . N   ไม่มีการ ปราบปราม การ แบ่งแยก ในภูมิภาค และ การทํา นาย ขอบเขต ของ วัตถุ ไม่ ซ้ํา ซ้อน  -   วัตถุประสงค์ นี้ มี การพัฒนา อย่างมีนัยสําคัญ ใน วัตถุ  -  มี การพัฒนา ที่สําคัญ หลายอย่าง เช่น การ อบรม แบบ ช้า และการ อนุ ให้ ภาพรวม ที่ ครอบคลุม ของ เทคนิค การ ตรวจจับ วัตถุ ใน  S . A . C . C .   กล่าวถึง ข้อ  2   กล่าวถึง ความท้าทาย ที่พบ ใน การตรวจสอบ วัตถุ ใน  G . S . A . C .   เน้น ปัจจัย ที่ มีส่วน ช่วยในการ ประเมิน ความ ซับซ้อน ของ ปัญหา  มาตรา  3   นําเสนอ การตรวจสอบ ชุด ข้อมูล ที่ใช้ กัน ทั่วไป สําหรับ เทคนิค การ ตรวจจับ วัตถุ ใน  M IS . S . C .  มาตรา  5   นําเสนอ สถานะ ของ ข้อมูล การ ตรวจจับ วัตถุ ที่ ได้ เปรียบ ต่าง  ๆ   ที่ จัดทํา ขึ้น โดย เน้น จุด แข็ง และ จุด อ่อน ของพวกเขา ในแง่ของ การคํานวณ และ ความทนทาน ในรูปแบบ ของ ข้อมูล  I . C . S .  มาตรา  6   สรุป กระดาษ โดย เน้น บาง คําถาม การวิจัย แบบ เปิด และ ทิศทาง ในอนาคต ในสาขา การ ตรวจจับ วัตถุ ใน   e . W . O . T . 2  Ch all es  2  Ch all es  in  Ch ang es  D et ection  2.1  O b j ect ive  D et ection   ในการ ก ฏ หมาย ความ เปลี่ยนแปลง ของ แสง  เช่น   เงา  และการ ระบุ ตัว แปร ปร ก ติ สามารถ ส่งผลกระทบ อย่างมีนัยสําคัญ ในการ ปรากฏตัว ของ   e . O . O .   ทําให้เกิด การ อัล ก อริ ท ึ ม ที่จะ ล้มเหลว ในการ รับรู้ ถึง วัตถุ หรือ ผลิต ข้อมูล บริบท ของ กล่อง ที่ ไม่ถูกต้อง โดย รอบ   การตรวจสอบ  O b j ect  เป็น งาน สําคัญ ใน การมองเห็น คอมพิวเตอร์ ที่เกี่ยวข้องกับ การ ปรากฏตัว และ ตําแหน่ง ของ วัตถุ ใน ภาพ หรือ วิดีโอ วิดีโอ ต่างๆ  เพื่อ ประเมิน ประสิทธิภาพ ของ เทคนิค การ ตรวจจับ วัตถุ ใน   ข้อมูล มาตรฐาน หลาย ชุด ข้อมูล มี ชุด ข้อมูล อ้างอิง หลาย ชุด ข้อมูล เหล่านี้ เป็น ชุด ข้อมูล ที่ได้รับ มาตรฐาน ที่ให้ ชุด ข้อมูล ที่ถูกต้อง ของ ภาพ ที่มี ข้อความ ระบุว่า สามารถ เปรียบเทียบ ความถูกต้อง และ ความเร็ว ของ ข้อมูล ที่แตกต่างกัน ชุด ข้อมูล ที่ ได้รับความนิยม บางส่วน  M as as ay  3.1  V   OC  The  M as as is  V OC  V OC   IS C  O b j ect   TM   ข้อมูล ชุด ข้อมูล เป็นหนึ่งใน ชุด ข้อมูล ที่ เก่าแก่ ที่สุด และ เป็นที่นิยม ที่สุด สําหรับ วัตถุ  D . C .  ประกอบด้วย ภาพ   e ff ect   หนึ่ง ชุด ข้อมูล ที่ ประกอบด้วย ชุด ข้อมูล ที่ เก่าแก่ ที่สุด และ เป็นที่นิยม ที่สุด สําหรับ วัตถุ  20   อย่าง เช่น  F la g  และ   ข้อมูล ชุด ข้อมูล ที่ให้ กล่อง บันทึก บันทึก สําหรับ วัตถุ แต่ละ วัตถุ ใน  F . A .  V OC   ได้ ใช้เป็น ชุด ข้อมูล เกณฑ์ มาตรฐาน สําหรับ O b j ect s  in  the  O p en  I m b j ect   ข้อมูล ชุด ข้อมูล ใน  CO CO  เป็น ชุด ข้อมูล ที่ ใหม่ กว่า  33 0,000   ภาพ  ด้วย  80   วัตถุ  DE CO  ให้ รายละเอียด คําอธิบาย ประกอบ ที่ มากกว่า  80   บทความ   รวมไปถึง การแบ่ง ส่วน แผ่น หน้ากาก สําหรับ แต่ละ วัตถุ ใน   LE A CO  ทําให้ ชุด ข้อมูล ต่างๆ   ที่ ท้าทาย ยิ่งขึ้น สําหรับ อัล ก อริ ท ึ ม การ ตรวจจับ วัตถุ   ช่วยในการ ทํางาน ได้เป็นอย่างดี   ข้อมูล ชุด ข้อมูล เป็น ชุด ข้อมูล ขนาดใหญ่ อีก ชุด หนึ่ง ที่ ประกอบด้วย  1.7  ล้าน ภาพ  ด้วย  600   วัตถุ   ช่วยในการ แบ่ง ส่วน ของ กล่อง และ แบ่ง ส่วน   mas   k   ได้ รวม เป็น มาตรฐาน สําหรับ อัล ก อริ ท ึ ม การ ตรวจจับ วัตถุ ที่ต้องการ ข้อมูล จํานวนมาก   รายชื่อ ผู้ เข้า รหัส การ ฝึกฝน   ข้อมูล เหล่านี้ ประกอบด้วย ชุด ข้อมูล ที่แตกต่างกัน จํานวน   ดังนี้  1.   ป้อน ข้อมูล สําคัญ บางอย่าง เกี่ยวกับ ชุด ข้อมูล มาตรฐาน ที่ ได้รับความนิยม สําหรับ ประเมิน ผล การ ตรวจจับ วัตถุ  2.   ตาราง  1.   สรุป ข้อมูล ที่สําคัญ เกี่ยวกับ ชุด ข้อมูล มาตรฐาน สําหรับ การตรวจสอบ วัตถุ  L . C . S .  จํานวน ภาพ ของ ข้อมูล  จํานวน ชั้น   ข้อมูล   ข้อมูล เครือข่าย ประสาท แบบ   sy s tem s   คือการ จําแนก และ ปรับแต่ง โครงสร้าง เหล่านี้ เป็น สอง ส่วน ของ  R   sy s tem s  ประกอบด้วย  1   ภูมิภาค  1.  ขั้นตอน แรกของ  R   e ff ect er   ช่วยสร้าง ภูมิภาค วัตถุ โดย การใช้ อัล ก อริ ท ึ ม การค้นหา ที่ รวม ระหว่าง อาหาร ที่ อาหาร ติดขัด เช่น สี และ อาหาร อาหาร ต่างๆ   กับ  S el ect ive   e ff ect ive   อย่าง เช่น ขอบ และ ปัญหา ใน เชิงลบ   การค้นหา จะ สร้าง ข้อเสนอ ของ ภูมิภาค ประมาณ   2,000   แบบ สําหรับ แต่ละ ภูมิภาค  2 .   คุณลักษณะ  1.   ข้อ ที่สอง   ข้อเสนอ ของ  R   แต่ละ ภูมิภาค จะ บิด เบี้ย ว ให้ มีขนาด คงที่ และ อาหาร ผ่าน เว ก เตอร์ ก่อน การ จําแนก ประเภท วัตถุ  3 .  การ จําแนก ประเภท  O b j ect   ex e ff ect  และ  Tr ans m ul ator   คุณลักษณะ ของ เว ก เตอร์ สําหรับ แต่ละ ภูมิภาค นั้น จะถูก ป้อน เข้าไปใน ชุด ของ ชั้น รวม เต็มรูปแบบ เพื่อ จัด หมวดหมู่ วัตถุ และ ขอบเขต ของ กล่อง  การ จําแนก ชั้น   ผลลัพธ์ ของ ความ น่าจะเป็น ของ แต่ละ ภูมิภาค ที่มี วัตถุ โดยเฉพาะ ในขณะที่ ชั้น การ ถดถอย ของ ขอบเขต การ ถดถอย ขอบเขต ขอบเขต และ ขอบเขต ของ วัตถุ แต่ละ กล่อง คือ  4 . 2.2  S SD  D et ect or  S ing le  Sh ot  D et ect or   ขยาย แนวคิด ของ ความ หลากหลาย โดยการ ทํา นาย ขอบเขต ขอบเขต และ ความ น่าจะเป็น ของ วัตถุ ที่ ต่างกัน  ซึ่ง ช่วยเพิ่ม การ ตรวจจับ วัตถุ ด้วย  S SD   ที่แตกต่างกัน ใช้ คุณสมบัติ แยก คุณลักษณะ ในการสร้าง แผนที่ และ นํา ตัวกรอง   tr ap  เพื่อ คาดการณ์ คะแนน ชั้น และ คะแนน ระดับ   91   แต่ละ กล่อง ประกอบด้วย  1.  S SD   เป็นตัว บ่งชี้ ความเร็ว และ การใช้งาน ใน การตรวจสอบ วัตถุ   r ock p at ent  re sp ir ect or  R et ina   Net  R et ina   แต่ละ กล่อง ใช้ เพื่อ ตรวจจับ ความเร็ว และ ประสิทธิภาพ ที่แท้จริง  -  เวลา ในการ ตรวจจับ วัตถุ  -  เวลา ในการ ตรวจจับ ความ ไม่ สมดุล ของ ข้อมูล จํา เพาะ ซึ่ง ช่วย แก้ปัญหา ความ ไม่ สมดุล ของ ชน ชั้น โดย การลด สัดส่วน ของ ตัวอย่าง ง่าย และการ เน้น ตัวอย่าง ที่ ยาก ในระหว่าง การ ตรวจจับ   ผลลัพธ์ ที่ได้รับ การปรับปรุง ให้ ดีขึ้น โดย เฉพาะอย่างยิ่ง สําหรับ  R et ina   ขนาดเล็ก  ใช้ ฟังก์ชั่น การ โฟกัส นวนิยาย ที่ กําหนด น้ําหนัก ที่ สูงกว่า ให้กับ ตัวอย่าง แข็ง และ ช่วยลด ผลกระทบ ของ ตัวอย่าง ง่าย  ๆ   ระหว่าง  R et ina ตาราง การ เปรียบเทียบ เทคนิค การ ตรวจจับ วัตถุ เกี่ยวกับ ชุด ข้อมูล  CO CO  วิธีการ ใช้ ค่าเฉลี่ย  Pre ci s ion  F ast  A 5  5 3.3  0.5  F ast  R  7 0.0  5  F aster  R  7  7 3.2  7  7 2.1  7 2.1  19  7 4.8  7 4.8  7 4.8  7 4.8  7 4.8 เบอร์  12   ผลลัพธ์ ใน ตาราง ที่  1   แสดงว่า สอง จุด พร้อมกัน เช่น  F aster  R  B ar g ing s   โดยทั่วไป ให้ ความแม่นยํา เฉลี่ย สูงกว่า เมื่อเทียบกับ อุปกรณ์ ตรวจจับ แบบ ตัว ตรวจจับ เดียว เช่น ตัว ตรวจจับ  F aster  R   s ch ar ge s   และอุปกรณ์ ตรวจจับ ชุด ข้อมูล เดียว ได้ เร็วขึ้น ในแง่ของ กรอบ ต่อ วินาที  -  เวลา  3 .  การ เปรียบเทียบ เทคนิค การ ตรวจจับ วัตถุ กับ ชุด ข้อมูล  CO CO   จริง  -  เวลา  3 .   สํารวจ การ ทํางานร่วมกัน ของ เซ็นเซอร์ อื่น  ๆ  เช่น   ข้อมูล หรือ ความลึก ของ วัตถุ   สรุป  เพื่อ เสริมสร้าง การ ตรวจจับ วัตถุ •   เกิด ความแตกต่าง หรือ เทคนิค การ ตรวจจับ วัตถุ อ่อน  ๆ   เพื่อลด การ พึ่งพา คําอธิบาย ประกอบ ภาพ ขนาดใหญ่  ( คําอธิบาย ภาพ )   ที่ เราใช้ โดย จัดการกับ ความท้าทาย เหล่านี้ และ สํารวจ วัตถุ ใหม่  ๆ  เรา เชื่อว่า วัตถุ ที่ ตรวจจับ ได้ใน สภาพแวดล้อม ที่ 1.   ไซ ่ง ่อ น มี วิธีการ ตรวจสอบ วิธีการ ใช้ วิธีการ ต่างๆ   ที่ น่าเชื่อถือ และ มีประสิทธิภาพ มากขึ้น   s im ple ment s .  เป็น วิธีการ ที่ มาจาก มา ส ก์  R   TM 2 . 2 . A 2 .   ro ad al   et   ro ff ect ive   s el ect or   s ens it ive s . 2 .  มี วิธีการ ตรวจจับ วัตถุ โดย อาศัย มา ส ก์  R   TM 2 . 2  R . C . 2  R .  และ  O t ad ul ata   ข้อมูล และ เอกสาร อ ภิ พันธุ์ ที่ถูก สแกน จาก เอกสาร อา รบ ิก โดยใช้ เว ก เตอร์  V er v ide as   TM 2 68   v 4 . 4  W .   st re am  และ  Q .   ศึกษา การ ตรวจจับ วัตถุ จาก  R   ได้ เร็วขึ้น  ในปี  20 1  7   ไช นี ส วิ ท อัตโนมัติ  C ong ress s   la f on  G im on   TM  5 . A .   เริ่ม กระบวนการ การ ส่องสว่าง และการ สะท้อน ของ วิธีการ สร้าง ภาพ ภายใน ภาพ ให้ ดี ยิ่งขึ้น  6 .   .  7 .   s . 13 . 13  H 13   . 13   . 13  13 . 13   .  13 . 13   . 13   . 13 . 13   . . 13 .   ผล ของ การปรับปรุง เทคนิค ความ เปิด ไม่ได้ ในการ ตรวจจับ วัตถุ   n ot ation s  in  Pro ce ed ing s   ของ  Pro ce ed ing s  Con fer ence   on  C om put er  Vi s ion  and  ลาย  H um an  14  14  H . E .  14   ได้ และ ให้ ภาพ ที่ เปิด ให้ ภาพ ชุด ข้อมูล ทั้งหมด เป็น ชุด ข้อมูล และ ชุด ข้อมูล ทั้งหมด   ตรวจจับ ความสัมพันธ์ ระหว่าง กัน ได้ ที่  K . M . S .  F . 19 56  H . 15  15 15  F 15  และ  15 15 . 15  F 15   ได้รับการ ตรวจสอบ และ ตรวจสอบ การ ตรวจจับ วัตถุ โดย อาศัย ส่วน ขอบ และการ พยากรณ์ วัตถุ  -  16 16  M 16  M 16   ตรวจจับ  O b j an n ot  และ จดจํา วัตถุ โดยใช้ ขอบ ฐาน และ เร็ว  R  R . G . 4 21 21  M . R . 4 21 21 21  S 1 17   PR 1 17   และอุปกรณ์ ตรวจจับ วัตถุ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R-MfEln3cfX",
        "outputId": "73fc0fae-97f4-4837-f528-310b80444787"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "omCHk-X13tGH",
        "outputId": "e02ce8c6-8e51-459e-dc30-178b87e86a93"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
            "Installing collected packages: numpy, scipy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.0.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.0.0 which is incompatible.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.4.0 which is incompatible.\n",
            "langchain 0.2.6 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.0 which is incompatible.\n",
            "langchain-community 0.2.6 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.0 which is incompatible.\n",
            "numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.0 which is incompatible.\n",
            "onnxruntime 1.18.1 requires numpy<2.0,>=1.21.6, but you have numpy 2.0.0 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.0.0 which is incompatible.\n",
            "unstructured 0.14.9 requires numpy<2, but you have numpy 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.0 scipy-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjz5m_su4C-j",
        "outputId": "a8dbf731-1635-4a06-e54e-c982120c0be6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall numpy nltk\n",
        "# !pip install numpy nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ifiCPRR67OLX",
        "outputId": "ec3bc4e6-19db-4af9-bf83-e4f529a363ae"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.0\n",
            "Uninstalling numpy-2.0.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/numpy-config\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy-2.0.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libgfortran-040039e1-0352e75f.so.5.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libquadmath-96973f99-934c22de.so.0.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libscipy_openblas64_-99b71e71.so\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy/*\n",
            "Proceed (Y/n)? n\n",
            "Found existing installation: nltk 3.8.1\n",
            "Uninstalling nltk-3.8.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/nltk\n",
            "    /usr/local/lib/python3.10/dist-packages/nltk-3.8.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/nltk/*\n",
            "Proceed (Y/n)? n\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/requirements.txt --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "fQ5vm0JC7q5S",
        "outputId": "3f360be4-d7a9-4903-afb7-fcc030f47acd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.21.0 (from -r /content/requirements.txt (line 1))\n",
            "  Using cached numpy-1.21.0-cp310-cp310-linux_x86_64.whl\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 2)) (2.0.3)\n",
            "Collecting pandas>=1.3.0 (from -r /content/requirements.txt (line 2))\n",
            "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 3)) (3.8.4)\n",
            "Collecting matplotlib (from -r /content/requirements.txt (line 3))\n",
            "  Using cached matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 4)) (1.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 6)) (2.32.3)\n",
            "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pandas>=1.3.0 (from -r /content/requirements.txt (line 2))\n",
            "  Using cached pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "  Using cached pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "  Using cached pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "  Using cached pandas-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "  Using cached pandas-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "  Using cached pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "  Using cached pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->-r /content/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->-r /content/requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->-r /content/requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 3)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 3)) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 3)) (1.4.5)\n",
            "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 3)) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 3)) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 3)) (3.1.2)\n",
            "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scipy<2.0.0,>=1.7.0 (from -r /content/requirements.txt (line 4))\n",
            "  Using cached scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
            "  Using cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "  Using cached scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "  Using cached scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "  Using cached scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached scipy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "  Using cached scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "  Using cached scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r /content/requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r /content/requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r /content/requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r /content/requirements.txt (line 6)) (2024.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->-r /content/requirements.txt (line 2)) (1.16.0)\n",
            "Installing collected packages: numpy, scipy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.0\n",
            "    Uninstalling numpy-2.0.0:\n",
            "      Successfully uninstalled numpy-2.0.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.0\n",
            "    Uninstalling scipy-1.14.0:\n",
            "      Successfully uninstalled scipy-1.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.21.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.21.0 which is incompatible.\n",
            "flax 0.8.4 requires numpy>=1.22, but you have numpy 1.21.0 which is incompatible.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.4.0 which is incompatible.\n",
            "jax 0.4.26 requires numpy>=1.22, but you have numpy 1.21.0 which is incompatible.\n",
            "jaxlib 0.4.26+cuda12.cudnn89 requires numpy>=1.22, but you have numpy 1.21.0 which is incompatible.\n",
            "ml-dtypes 0.2.0 requires numpy>=1.21.2; python_version > \"3.9\", but you have numpy 1.21.0 which is incompatible.\n",
            "numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 1.21.0 which is incompatible.\n",
            "numexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.21.0 which is incompatible.\n",
            "onnxruntime 1.18.1 requires numpy<2.0,>=1.21.6, but you have numpy 1.21.0 which is incompatible.\n",
            "opencv-contrib-python 4.8.0.76 requires numpy>=1.21.2; python_version >= \"3.10\", but you have numpy 1.21.0 which is incompatible.\n",
            "opencv-python 4.8.0.76 requires numpy>=1.21.2; python_version >= \"3.10\", but you have numpy 1.21.0 which is incompatible.\n",
            "opencv-python-headless 4.10.0.84 requires numpy>=1.21.2; python_version >= \"3.10\", but you have numpy 1.21.0 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.21.0 which is incompatible.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.21.0 which is incompatible.\n",
            "pywavelets 1.6.0 requires numpy<3,>=1.22.4, but you have numpy 1.21.0 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.21.0 which is incompatible.\n",
            "statsmodels 0.14.2 requires numpy>=1.22.3, but you have numpy 1.21.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.21.0 which is incompatible.\n",
            "xarray-einstats 0.7.0 requires numpy>=1.22, but you have numpy 1.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.21.0 scipy-1.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "c0de63fd739c4a1b89e546cd1d951c57"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8SdupvE3V9L",
        "outputId": "0d46f3bf-4a39-406d-f6b8-c401f564640c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### en -> th, word -> bpe Transformer Base Model"
      ],
      "metadata": {
        "id": "mp9qrcHqz0CW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en2th_word2bpe = TransformerModel.from_pretrained(\n",
        "                    model_name_or_path='mt/SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0/models/',\n",
        "                    checkpoint_file='checkpoint.pt',\n",
        "                    data_name_or_path='mt/SCB_1M+TBASE_en-th_moses-spm_130000-16000_v1.0/vocab/'\n",
        ")"
      ],
      "metadata": {
        "id": "dBhT1eksz4zc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --editable ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "uEHkgEixBGzh",
        "outputId": "f47dc7b0-31a1-42bd-f34c-a8e5d33ff21d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (3.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (1.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (2024.5.15)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (2.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.9.0) (4.66.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.9.0) (2.22)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->fairseq==0.9.0) (2.10.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->fairseq==0.9.0) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->fairseq==0.9.0) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->fairseq==0.9.0) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.9.0) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq==0.9.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq==0.9.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq==0.9.0) (1.3.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-0.editable-cp310-cp310-linux_x86_64.whl size=7597 sha256=7d0aa0f49e849c9cab6e3757fce30408042f353922463fd077c22746b7152f12\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ayne0j8e/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "Successfully built fairseq\n",
            "Installing collected packages: fairseq\n",
            "  Attempting uninstall: fairseq\n",
            "    Found existing installation: fairseq 0.12.2\n",
            "    Uninstalling fairseq-0.12.2:\n",
            "      Successfully uninstalled fairseq-0.12.2\n",
            "Successfully installed fairseq-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fairseq"
                ]
              },
              "id": "5d5445465ec54a999bc931b89ee525ce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = text.replace('▁', '')\n",
        "    return text"
      ],
      "metadata": {
        "id": "tfk8N_lvDTv0"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_chunk = chunks[0].page_content\n",
        "sentences = sent_tokenize(first_chunk)\n",
        "\n",
        "translated_sentences = []\n",
        "for sentence in sentences:\n",
        "    translated = en2th_word2bpe.translate(sentence)\n",
        "    cleaned_translation = clean_text(translated)\n",
        "    translated_sentences.append(cleaned_translation)"
      ],
      "metadata": {
        "id": "hucsiEi9GP0p"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_chunk = ' '.join(translated_sentences)\n",
        "\n",
        "print(\"Original :\\n\", first_chunk)\n",
        "print(\"\\nTranslated :\\n\", translated_chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiFXARQ4GUql",
        "outputId": "5d2a5dda-0469-4278-f87e-ee454c0b6e23"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original :\n",
            " A Comprehensive Study on Object Detection Techniques in \n",
            "Unconstrained Environments  \n",
            " \n",
            "Hrishitva Patel1 \n",
            "1The University of Texas at San Antonio   \n",
            "hrishitva.patel@my.utsa.edu   \n",
            " \n",
            "Abstract  \n",
            "Object detection is a crucial task in computer vision that aims to identify and localize objects in \n",
            "images or videos. The recent advancements in deep learning and Convolutional Neural Networks \n",
            "(CNNs) have significantly improved the performance of object detection techniques. This paper \n",
            "presents a comprehensive study of object detection techniques in unconstrained environments, \n",
            "including various challeng es, datasets, and state -of-the-art approaches. Additionally, we present a \n",
            "comparative analysis of the methods and highlight their strengths and weaknesses. Finally, we \n",
            "provide some future research directions to further improve object detection in unconstra ined \n",
            "environments.  \n",
            "Keywords:  object detection, unconstrained environments, deep learning, convolutional neural \n",
            "networks, computer vision  \n",
            "1 Introduction  and Background  \n",
            "Object detection is a fundamental problem in computer vision, with numerous applications \n",
            "spanning fields such as surveillance, robotics, autonomous vehicles, augmented reality, and \n",
            "human -computer interaction. The primary goal of object detection is to recognize and localize \n",
            "instances of objects belonging to predefined classes in images or vide os. In recent years, significant \n",
            "progress has been made in the development of object detection algorithms, mainly due to the \n",
            "emergence of deep learning and Convolutional Neural Networks (CNNs). These advancements \n",
            "have led to impressive performance improvem ents in various benchmark datasets, such as \n",
            "PASCAL VOC, ImageNet, and MS COCO. Despite these successes, object detection in \n",
            "unconstrained environments remains a challenging task. Unconstrained environments are \n",
            "characterized by variations in lighting condit ions, viewpoint changes, occlusions, object\n",
            "\n",
            "Translated :\n",
            "  การศึกษา ที่ ครอบคลุม เกี่ยวกับ เทคนิค การ ตรวจจับ วัตถุ ในรูปแบบ พารา มิเตอร์ พารา มิเตอร์ พารา มิเตอร์  มหาวิทยาลัย การ ก  า หน ด พารา มิเตอร์  ของ เท็กซัส ที่ การตรวจสอบ วัตถุ คัด ย่อ ใน ซาน อัน โต นิ โอ  คอร์ ป อเร ชัน เป็น ภารกิจ สําคัญ ใน วิสัยทัศน์ ทาง คอมพิวเตอร์ ที่มี จุดมุ่งหมาย ในการ ระบุ และ ระบุ วัตถุ ต่าง ๆ ใน ภาพ หรือ การแสดง ออก  ความก้าวหน้า เมื่อเร็ว ๆ  นี้ ใน การพัฒนา เครือข่าย ประสาท เทียม ( N e ur al Network s ) และ  ne ur al  ne ur al  เครือข่าย ประสาท ( N e ur al Network s )  ได้ ปรับปรุง ประสิทธิภาพ การ ตรวจจับ วัตถุ อย่างมีนัยสําคัญ กระดาษ นี้ นําเสนอ การศึกษา ที่ ครอบคลุม ของ เทคนิค การ ตรวจจับ วัตถุ ใน  e . t . com รวมถึง p ro t ection s  ต่าง ๆ  co . th ro up s และ  st ate  st ate . com เรา นําเสนอ การวิเคราะห์ เปรียบเทียบ ของ วิธีการ และ ไฮไลท์ จุด แข็ง และ จุด แข็ง ของพวกเขา เรา ให้ คําแนะนํา การวิจัย ในอนาคต เพื่อ ปรับปรุง การ ตรวจจับ วัตถุ ใน S al ley  ขึ้น จริง  มุมมอง ทาง คอมพิวเตอร์  การมองเห็น 1  บท นํา และการ ตรวจจับ วัตถุ ทาง วัตถุ เป็น ปัญหา พื้นฐาน ใน คอมพิวเตอร์  app lic ation s  ที่มี แอพพลิเคชั่น มากมาย ครอบคลุม สาขา ต่าง ๆ เช่น  di ff ect  sy s tem s  อัตโนมัติ  sy s tem s  sy s tem s  sy s tem s  ro up press ion s และ p ro t ection  ของมนุษย์  เป้าหมาย หลักของ การตรวจสอบ วัตถุ คือการ จดจํา และ ระบุ กรณี ของ วัตถุ ที่อยู่ใน ชั้นเรียน ของ  ที่กําหนด ไว้ ล่วงหน้า ใน ภาพ หรือ ข้อมูล  .  ความคืบหน้า ที่สําคัญ ใน การพัฒนา ระบบ ตรวจจับ วัตถุ เป็น ส่วนใหญ่ เนื่องจาก การ เกิดขึ้น ของ เครือข่าย ประสาท แบบ  ne ur al  ne ur al  เครือข่าย ประสาท ( N e ur al Network s )  ความก้าวหน้า เหล่านี้ ได้ นําไปสู่ ประสิทธิภาพ ที่ น่าประทับใจ  tr ans lu  con tr oll s ใน เกณฑ์ มาตรฐาน ต่าง ๆ  อย่าง เช่น  tr et ro  v ang u k  st re และ  MS  v el op . com  แม้จะมี การ ตรวจจับ วัตถุ เหล่านี้ ใน สภาพแวดล้อม ที่ ยาก  สภาพแวดล้อม การ ก พ หน ด แบบ พารา มิเตอร์ จะ มีลักษณะ แปร ผั น แปร ใน วัตถุ แสง แบบ ไม่ จํากัด ขอบเขต\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0fVjbLoGGUoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1oDMtEp5GUmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "\n",
        "translated_sentences = []\n",
        "for sentence in sentences:\n",
        "    translated = en2th_word2bpe.translate(sentence)\n",
        "    cleaned_translation = clean_text(translated)\n",
        "    translated_sentences.append(cleaned_translation)"
      ],
      "metadata": {
        "id": "A64DIknIDTtf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_translated_text = ' '.join(translated_sentences)\n",
        "\n",
        "print(\"ข้อความต้นฉบับ:\\n\", text)\n",
        "print(\"\\nข้อความที่แปลแล้ว:\\n\", final_translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcQ8U1kkDTrH",
        "outputId": "b281b875-e1ae-4fd5-8c8a-2f9dc5b62936"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ข้อความต้นฉบับ:\n",
            " A Comprehensive Study on Object Detection Techniques in \n",
            "Unconstrained Environments  \n",
            " \n",
            "Hrishitva Patel1 \n",
            "1The University of Texas at San Antonio   \n",
            "hrishitva.patel@my.utsa.edu   \n",
            " \n",
            "Abstract  \n",
            "Object detection is a crucial task in computer vision that aims to identify and localize objects in \n",
            "images or videos. The recent advancements in deep learning and Convolutional Neural Networks \n",
            "(CNNs) have significantly improved the performance of object detection techniques. This paper \n",
            "presents a compre\n",
            "\n",
            "ข้อความที่แปลแล้ว:\n",
            "  การศึกษา ที่ ครอบคลุม เกี่ยวกับ เทคนิค การ ตรวจจับ วัตถุ ในรูปแบบ พารา มิเตอร์ พารา มิเตอร์ พารา มิเตอร์  มหาวิทยาลัย การ ก  า หน ด พารา มิเตอร์  ของ เท็กซัส ที่ การตรวจสอบ วัตถุ คัด ย่อ ใน ซาน อัน โต นิ โอ  คอร์ ป อเร ชัน เป็น ภารกิจ สําคัญ ใน วิสัยทัศน์ ทาง คอมพิวเตอร์ ที่มี จุดมุ่งหมาย ในการ ระบุ และ ระบุ วัตถุ ต่าง ๆ ใน ภาพ หรือ การแสดง ออก  ความก้าวหน้า เมื่อเร็ว ๆ  นี้ ใน การพัฒนา เครือข่าย ประสาท เทียม ( N e ur al Network s ) และ  ne ur al  ne ur al  เครือข่าย ประสาท ( N e ur al Network \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGr_g-oDFBhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AL5mB8Y_FBfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0l-U6Tr2FBdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def sentence_splitter(text, max_chunk_size=2000):\n",
        "#     sentences = nltk.sent_tokenize(text)\n",
        "#     chunks = []\n",
        "#     current_chunk = ''\n",
        "\n",
        "#     for sentence in sentences:\n",
        "#         if len(current_chunk) + len(sentence) <= max_chunk_size:\n",
        "#             current_chunk += ' ' + sentence\n",
        "#         else:\n",
        "#             chunks.append(current_chunk.strip())\n",
        "#             current_chunk = sentence\n",
        "\n",
        "#     if current_chunk:\n",
        "#         chunks.append(current_chunk.strip())\n",
        "\n",
        "#     return chunks"
      ],
      "metadata": {
        "id": "3036hhgQBjck"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_chunks = []\n",
        "# for document in documents:\n",
        "#     all_chunks.extend(sentence_splitter(document.page_content))\n",
        "\n",
        "# # Translate chunks\n",
        "# translated_chunks = [en2th_word2bpe.translate(chunk) for chunk in all_chunks]\n",
        "\n",
        "# # Reassemble translated chunks\n",
        "# translated_text = ' '.join(translated_chunks)\n",
        "\n",
        "# # Print the original and translated text\n",
        "# print(\"Original Text:\\n\", text)\n",
        "# print(\"\\nTranslated Text:\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nlnaDYTNBnyo",
        "outputId": "38fe3de7-574a-44af-b526-dba258671d2d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " A Comprehensive Study on Object Detection Techniques in \n",
            "Unconstrained Environments  \n",
            " \n",
            "Hrishitva Patel1 \n",
            "1The University of Texas at San Antonio   \n",
            "hrishitva.patel@my.utsa.edu   \n",
            " \n",
            "Abstract  \n",
            "Object detection is a crucial task in computer vision that aims to identify and localize objects in \n",
            "images or videos. The recent advancements in deep learning and Convolutional Neural Networks \n",
            "(CNNs) have significantly improved the performance of object detection techniques. This paper \n",
            "presents a comprehensive study of object detection techniques in unconstrained environments, \n",
            "including various challeng es, datasets, and state -of-the-art approaches. Additionally, we present a \n",
            "comparative analysis of the methods and highlight their strengths and weaknesses. Finally, we \n",
            "provide some future research directions to further improve object detection in unconstra ined \n",
            "environments.  \n",
            "Keywords:  object detection, unconstrained environments, deep learning, convolutional neural \n",
            "networks, computer vision  \n",
            "1 Introduction  and Background  \n",
            "Object detection is a fundamental problem in computer vision, with numerous applications \n",
            "spanning fields such as surveillance, robotics, autonomous vehicles, augmented reality, and \n",
            "human -computer interaction. The primary goal of object detection is to recognize and localize \n",
            "instances of objects belonging to predefined classes in images or vide os. In recent years, significant \n",
            "progress has been made in the development of object detection algorithms, mainly due to the \n",
            "emergence of deep learning and Convolutional Neural Networks (CNNs). These advancements \n",
            "have led to impressive performance improvem ents in various benchmark datasets, such as \n",
            "PASCAL VOC, ImageNet, and MS COCO. Despite these successes, object detection in \n",
            "unconstrained environments remains a challenging task. Unconstrained environments are \n",
            "characterized by variations in lighting condit ions, viewpoint changes, occlusions, object \n",
            "deformations, scale changes, and the presence of cluttered backgrounds. These factors can severely \n",
            "affect the performance of object detection algorithms, making it difficult to achieve high detection \n",
            "accuracy and  robustness.   \n",
            "In recent years, significant progress has been made in object detection, particularly in the area of \n",
            "deep learning and Convolutional Neural Networks (CNNs)  [1]. These techniques have \n",
            "significantly improved the performance of object detection algorithms, particularly in \n",
            "unconstrained environments where objects may appear at different scales, angles, and orientations.  \n",
            "Region -based object detectors, such as Region -based Convolutional Neural Networks (R -CNN) \n",
            "[2], operate by first generating region proposals using a selective search algorithm, which generates around 2000 regions per image. Each region is then passed through a CNN to generate \n",
            "a fixed -length feature vector, which is fed into a support vector machine (SVM)  [3] to classify the \n",
            "region and predict its bounding box coordinates. Finally, non -maximum suppression is applied to \n",
            "eliminate redundant detections. While R -CNN was a significant breakthrough in object detection, \n",
            "it has several limitations, such as slow training and inference times.  \n",
            " \n",
            "To address these issues, researchers have proposed several variants of R -CNN, such as Fast R -\n",
            "CNN [4], which shares convolutional features across region proposals, and Faster R -CNN  , which \n",
            "introduces a Region Proposal Network (RPN) to generate region proposals in an end -to-end \n",
            "manner. These variants significantly improve the speed and accuracy of R -CNN, making it a \n",
            "popular choice for  object detection in unconstrained environments.  The purpose of this paper is to \n",
            "provide a comprehensive overview of object detection techniques in unconstrained environments, \n",
            "addressing the challenges, datasets, and state -of-the-art approaches.  \n",
            "The paper  is organized as follows: Section 2 discusses the challenges encountered in object \n",
            "detection in unconstrained environments, highlighting the factors that contribute to the complexity \n",
            "of the problem. Section 3 presents a review of the commonly used datasets  for evaluating object \n",
            "detection techniques in unconstrained environments. Section 4 presents state of the art Objection \n",
            "detection techniques. Section 5 presents a comparative analysis of the surveyed methods, \n",
            "emphasizing their strengths and weaknesses in terms of accuracy, computational complexity, and \n",
            "robustness to variations in the unconstrained environment. Section 6 concludes the paper by \n",
            "highlighting some of the open research questions and future directions in the field of object \n",
            "detection in unconstr ained environments.  \n",
            "2 Challenges in Object Detection in Unconstrained Environments  \n",
            "2.1 Illumination Changes  \n",
            "Variations in lighting conditions, such as shadows and overexposure, can significantly impact the \n",
            "appearance of objects, making it difficult for de tection algorithms to identify and localize them \n",
            "accurately. Variations in lighting conditions, such as shadows, overexposure, or underexposure, \n",
            "can significantly impact the appearance of objects in images [5]. These changes can make it \n",
            "difficult for detection algorithms to identify and localize objects accurately. To address this issue, \n",
            "several approaches have been proposed, including color constancy techniques [6] and deep \n",
            "learning -based methods that can learn illumination invariant features  [7].  \n",
            "2.2 Viewpoint  Variation  \n",
            "Changes in the viewpoint or camera angle can alter the object's appearance, causing the detection \n",
            "algorithm to fail in recognizing the object or produce inaccurate bounding boxes [8]. Several \n",
            "methods have been proposed to tackle this issue, such as viewpoint invariant features and multi -\n",
            "view object detectors  [8]. 2.3 Occlusion  \n",
            "Objects in the scene may be partially or entirely occluded by other objects, making it challenging \n",
            "for the detection algorithm to identify and localize t hem correctly [9]. To address occlusion, some \n",
            "methods employ part -based models [10] or leverage context information from surrounding regions.  \n",
            "3 Datasets  \n",
            "Object detection is a vital task in computer vision that involves identifying the presence and \n",
            "location of objects in an image or video. To evaluate the performance of object detection \n",
            "techniques in unconstrained environments, several benchmark datasets have been  created. These \n",
            "datasets provide a standardized set of images with labeled objects, enabling researchers to compare \n",
            "the accuracy and speed of different algorithms. Some popular datasets include:  \n",
            "3.1 Pascal V OC  \n",
            "The PASCAL VOC (Visual Object Classes) dataset is one of the oldest and most popular datasets \n",
            "for object detection. It contains 17,125 images with 20 object classes, such as person, car, and dog. \n",
            "The dataset provides bounding box annotations for each object in the image. PASCAL VOC has  \n",
            "been used as a benchmark dataset for several years, and many state -of-the-art object detection \n",
            "techniques have been evaluated on this dataset.  \n",
            "3.2 ImageNet  \n",
            "The ImageNet dataset is a massive dataset that contains 1.2 million images with 1,000 object \n",
            "classes. Unlike PASCAL VOC, ImageNet does not provide annotations for object detection. \n",
            "However, many researchers have used this dataset to pre -train their models on a large amount of \n",
            "data before fine -tuning them on smaller object detection datasets.  \n",
            "3.3 C OCO  \n",
            "The COCO (Common Objects in Context) dataset is a newer dataset that contains 330,000 images \n",
            "with 80 object classes. COCO provides more detailed annotations than PASCAL VOC, including \n",
            "segmentation masks for each object in the image. This makes COCO a more challenging dataset \n",
            "for object detection algorithms to perform well on.  \n",
            "3.4 Open Images  \n",
            "The Open Images dataset is another large -scale dataset that contains 1.7 million images with 600 \n",
            "object classes. It provides both bounding box and segmentation mas k annotations and  has been \n",
            "used as a benchmark for object detection algorithms that require large amounts of training data.  \n",
            "These datasets vary in size, number of classes, and annotation types, allowing researchers to test \n",
            "their algorithms on a wide range of scenarios. The following table summarizes some key \n",
            "information about the four popular benchmark datasets used for evaluating object detection \n",
            "techniques:  Table 1. Summary of key information about benchmark datasets for object detection  \n",
            "Dataset Name  Number of Images  Number of Classes  Annotation Type  \n",
            "PASCAL VOC [11] 17,125  20 Bounding Boxes  \n",
            "ImageNet  [12] 1.2 million  1,000  Bounding Boxes  \n",
            "COCO  [13] 330,000  80 Bounding Boxes  \n",
            "Open Images  [14] 1.7 million  600 Mask RCNN  \n",
            " \n",
            "4 State -of-the-art Object Detection Techniques  \n",
            "We categorize the state -of-the-art object detection techniques into two main groups: two -stage \n",
            "detectors and single -stage detectors.  \n",
            " \n",
            "Figure 1. Milestones of object detection  [15]. \n",
            "4.1 Two -stage detectors  \n",
            "Two-stage detectors consist of a region proposal stage followed by a classification stage. Some \n",
            "prominent two -stage detectors include:  \n",
            "4.1.1 R-CNN  \n",
            "R-CNN (Region -based Convolutional Neural Networks) is an object detection model that was \n",
            "proposed in 2014 by Ross Girshick et al. R -CNN is a two -stage object detection framework that \n",
            "uses a region proposal mechanism to generate potential object regio ns in an image and then applies \n",
            "a convolutional neural network (CNN) to classify and refine these regions.  \n",
            "The R -CNN framework consists of the following steps:  \n",
            "1. Region Proposal: The first stage of R -CNN generates potential object regions by using a \n",
            "selecti ve search algorithm that combines low -level features, such as color and texture, with \n",
            "high-level cues, such as edges and corners. Selective search generates around 2,000 region \n",
            "proposals for each image.  \n",
            "2. Feature Extraction: In the second stage, each region proposal is warped to a fixed size and \n",
            "fed through a pre -trained CNN, such as Alex Net  or VGG, to extract a feature vector for \n",
            "that region.  \n",
            "3. Object Classification and Refinement: The feature vector for each region proposal is then \n",
            "fed into a set of fully co nnected layers that perform object classification and bounding box \n",
            "regression. The classification layer outputs the probability of each region proposal \n",
            "containing a particular object class, while the regression layer outputs the refined bounding \n",
            "box coordi nates for that object class.  \n",
            "4.1.2 Fast R -CNN  \n",
            "Faster R -CNN (Region -based Convolutional Neural Networks): Faster R -CNN is a two -stage \n",
            "object detection model that uses a Region Proposal Network (RPN) to generate object proposals \n",
            "and a Fast R -CNN network to c lassify and refine the proposals. The RPN generates region \n",
            "proposals by sliding a small network over the convolutional feature map and predicting abjectness  \n",
            "scores and bounding box offsets. Faster R -CNN is known for its accuracy and has been widely \n",
            "used in  object detection tasks.  \n",
            "4.2 Single -stage detectors  \n",
            "Single -stage detectors directly predict object bounding boxes and class probabilities from an image. \n",
            "Some popular single -stage detectors include:  \n",
            "4.2.1 YOLO  \n",
            "YOLO is another one -stage object detection model that predicts object class scores and bounding \n",
            "box offsets directly from the entire image. YOLO divides the image into a grid of cells and predicts \n",
            "the class and bounding box for each cell. YOLO uses a single neural network to make predicti ons \n",
            "and is known for its speed and real -time performance.  \n",
            "4.2.2 SSD  \n",
            "The Single Shot MultiBox Detector (SSD) extends the concept of YOLO by predicting bounding \n",
            "boxes and class probabilities at multiple scales, which improves the detection of objects with \n",
            "varying sizes.  SSD uses a feature extractor to generate convolutional feature maps and applies a \n",
            "set of convolutional filters to predict class scores and offsets for each default box. SSD is known \n",
            "for its speed and efficiency and has been used in real-time object detection applications.  4.2.3 Retina Net  \n",
            "Retina Net  introduces the Focal Loss, which addresses the issue of class imbalance by down \n",
            "weighting  the contribution of easy examples and focusing on hard examples during training. This \n",
            "results in improved detection performance, particularly for small objects.  Retina Net uses a novel \n",
            "focal loss function that assigns higher weights to hard examples and reduces the effect of easy \n",
            "examples during training. Retina Net also uses a Feature Pyramid Network  (FPN) to handle objects \n",
            "at different scales and has achieved state -of-the-art performance on several object detection \n",
            "benchmarks.  \n",
            " \n",
            "Figure 2. One stage vs two stage object detection .  \n",
            "The below  table summarizes some key features of these state -of-the-art object detection techniques:  \n",
            "Table 2. Key features of state -of-the-art object detection techniques  \n",
            "Technique  Training Time  Inference Time  Number of Parameters  AP on COCO  \n",
            "Faster R -CNN  [16] Long  Medium  High  39.3 \n",
            "SSD [17] Medium  Fast Low 31.2 \n",
            "YOLO  [18] Short  Very Fast  Low 28.2 \n",
            "Retina Net  [19] Long  Medium  High  39.1 \n",
            " \n",
            "5 Comparative Analysis  \n",
            "In this section, we compare the performance of various object detection techniques on the COCO \n",
            "dataset [5]. The results are summarized in Table 1.  \n",
            "Table 1: Comparison of object detection techniques on the COCO dataset  \n",
            "Method  Average Precision (AP)  Speed (fps)  \n",
            "R-CNN 53.3 0.5 \n",
            "Fast R -CNN  70.0 5 \n",
            "Faster R -CNN  73.2 7 \n",
            "YOLOv3  57.9 45 \n",
            "SSD 72.1 19 \n",
            "RetinaNet  74.8 12 \n",
            " \n",
            "The results in Table 1 show that two -stage detectors, such as Faster R -CNN, generally achieve \n",
            "higher average precision (AP) compared to single -stage detectors like YOLOv3 and SSD. \n",
            "However, single -stage detectors are faster in terms of frames per second (fps), making them more \n",
            "suitable for real -time applications.  \n",
            " \n",
            "Figure 3. Comparison of object detection techniques on the COCO dataset  \n",
            " 6 Conclusion and Future Directions  \n",
            "In this paper, we have presented a comprehensive study on object detection techniques in \n",
            "unconstrained environments. We have discussed the challenges associated with object detection \n",
            "in suc h environments, presented popular datasets, and provided an overview of the state -of-the-art \n",
            "techniques. Additionally, we have compared the performance of various methods and highlighted \n",
            "their strengths and weaknesses.  \n",
            "Despite the significant progress made  in recent years, object detection in unconstrained \n",
            "environments remains a challenging problem. Future research directions could focus on the \n",
            "following aspects:  \n",
            "• Developing more robust algorithms capable of handling occlusions, lighting variations, and \n",
            "background clutter.  \n",
            "• Investigating techniques for efficient and accurate detection of small -scale objects.  \n",
            "• Exploring the integration of other sensor modalities, such as LiDAR or depth information, \n",
            "to enhance object detection performance.  \n",
            "• Developing unsupervised or weakly supervised object detection techniques to reduce the \n",
            "reliance on large -scale annotated datasets.  \n",
            "By addressing these challenges and exploring new approaches, we believe that object detection in \n",
            "unconstrained environments can be further improved, paving the way for more reliable and \n",
            "efficient applications in various domains, such as autonomous vehicles, robotics, and surveillance \n",
            "systems.  \n",
            "References  \n",
            " \n",
            "1. Dhillon, A. and G.K.J.P.i.A.I. Verma, Convolutional neural network: a review of models, \n",
            "methodologies and applications to object detection. 2020. 9(2): p. 85 -112. \n",
            "2. Xu, C., et al., A page object detection method based on mask R -CNN. 2021. 9: p. 143448 -\n",
            "143457. \n",
            "3. Qin, W., R. Elanwar, and M.J.J.o.I.S. Betke, Text and metadata extraction from scanned \n",
            "Arabic documents using support vector machines. 2022. 48(2): p. 268 -279. \n",
            "4. Liu, B., W. Zhao, and Q. Sun. Study of object detection based on Faster R -CNN. in 201 7 \n",
            "Chinese Automation Congress (CAC). 2017. IEEE.  \n",
            "5. Wong, J.K.W., et al., Development of a refined illumination and reflectance approach for \n",
            "optimal construction site interior image enhancement. 2022.  \n",
            "6. Li, Y., et al., A unified probabilistic framework of  robust and efficient color consistency \n",
            "correction for multiple images. 2022. 190: p. 1 -24. \n",
            "7. Csurka, G. and M.J.a.p.a. Humenberger, From handcrafted to deep local invariant features. \n",
            "2018. 2: p. 1.  \n",
            "8. Doi, K., et al., Detecting Object -Level Scene Changes  in Images with Viewpoint \n",
            "Differences Using Graph Matching. 2022. 14(17): p. 4225.  \n",
            "9. Cao, Z., et al., A Multi -Object Tracking Algorithm With Center -Based Feature Extraction \n",
            "and Occlusion Handling. 2022.  \n",
            "10. Somers, V., C. De Vleeschouwer, and A. Alahi. Bo dy Part -Based Representation Learning \n",
            "for Occluded Person Re -Identification. in Proceedings of the IEEE/CVF Winter \n",
            "Conference on Applications of Computer Vision. 2023.  \n",
            "11. Hwang, B., S. Lee, and H.J.E. Han, LNFCOS: Efficient Object Detection through Deep \n",
            "Learning Based on LNblock. 2022. 11(17): p. 2783.  12. Zhao, J., et al., Data -adaptive binary neural networks for efficient object detection and \n",
            "recognition. 2022. 153: p. 239 -245. \n",
            "13. Ma, J., Y. Ushiku, and M. Sagara. The effect of improving annotation qual ity on object \n",
            "detection datasets: A preliminary study. in Proceedings of the IEEE/CVF Conference on \n",
            "Computer Vision and Pattern Recognition. 2022.  \n",
            "14. Kuznetsova, A., et al., The open images dataset v4: Unified image classification, object \n",
            "detection, and v isual relationship detection at scale. 2020. 128(7): p. 1956 -1981.  \n",
            "15. Xiao, Y., et al., A review of object detection based on deep learning. 2020. 79: p. 23729 -\n",
            "23791.  \n",
            "16. Rani, S., et al., Object detection and recognition using contour based edge detectio n and \n",
            "fast R -CNN. 2022. 81(29): p. 42183 -42207.  \n",
            "17. Feroz, M.A., et al. Object detection and classification from a real -time video using SSD \n",
            "and YOLO models. in Computational Intelligence in Pattern Recognition: Proceedings of \n",
            "CIPR 2021. 2022. Springer.  \n",
            "18. Diwan, T., et al., Object detection using YOLO: Challenges, architectural successors, \n",
            "datasets and applications. 2022: p. 1 -33. \n",
            "19. Li, G., et al. Knowledge distillation for object detection via rank mimicking and prediction -\n",
            "guided feature imitation. i n Proceedings of the AAAI Conference on Artificial Intelligence. \n",
            "2022.  \n",
            " \n",
            "\n",
            "Translated Text:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first_chunk = chunks[0].page_content\n",
        "# translated_chunk = en2th_word2bpe.translate(first_chunk)\n",
        "\n",
        "# print(\"Original Text:\\n\", first_chunk)\n",
        "# print(\"\\nTranslated Text:\\n\")\n",
        "\n",
        "# translated_lines = translated_chunk.split('▁')\n",
        "# for line in translated_lines:\n",
        "#     print(line.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JChtCtP70Xol",
        "outputId": "2fbd4bc3-950b-4dbc-cfd6-20eaafdd4c9e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " A Comprehensive Study on Object Detection Techniques in \n",
            "Unconstrained Environments  \n",
            " \n",
            "Hrishitva Patel1 \n",
            "1The University of Texas at San Antonio   \n",
            "hrishitva.patel@my.utsa.edu   \n",
            " \n",
            "Abstract  \n",
            "Object detection is a crucial task in computer vision that aims to identify and localize objects in \n",
            "images or videos. The recent advancements in deep learning and Convolutional Neural Networks \n",
            "(CNNs) have significantly improved the performance of object detection techniques. This paper \n",
            "presents a comprehensive study of object detection techniques in unconstrained environments, \n",
            "including various challeng es, datasets, and state -of-the-art approaches. Additionally, we present a \n",
            "comparative analysis of the methods and highlight their strengths and weaknesses. Finally, we \n",
            "provide some future research directions to further improve object detection in unconstra ined \n",
            "environments.  \n",
            "Keywords:  object detection, unconstrained environments, deep learning, convolutional neural \n",
            "networks, computer vision  \n",
            "1 Introduction  and Background  \n",
            "Object detection is a fundamental problem in computer vision, with numerous applications \n",
            "spanning fields such as surveillance, robotics, autonomous vehicles, augmented reality, and \n",
            "human -computer interaction. The primary goal of object detection is to recognize and localize \n",
            "instances of objects belonging to predefined classes in images or vide os. In recent years, significant \n",
            "progress has been made in the development of object detection algorithms, mainly due to the \n",
            "emergence of deep learning and Convolutional Neural Networks (CNNs). These advancements \n",
            "have led to impressive performance improvem ents in various benchmark datasets, such as \n",
            "PASCAL VOC, ImageNet, and MS COCO. Despite these successes, object detection in \n",
            "unconstrained environments remains a challenging task. Unconstrained environments are \n",
            "characterized by variations in lighting condit ions, viewpoint changes, occlusions, object\n",
            "\n",
            "Translated Text:\n",
            "\n",
            "\n",
            "การศึกษา อย่าง ครอบคลุม เกี่ยวกับ เทคนิค การ ตรวจจับ วัตถุ ในรูปแบบ การ ก\n",
            "า หน ด พารา มิเตอร์\n",
            "University\n",
            "of\n",
            "T ex f or ce\n",
            "ของ เท็กซัส ที่ การตรวจสอบ วัตถุ ใน\n",
            "S an\n",
            "An ton io\n",
            "An ton io\n",
            "TM\n",
            "ถือเป็น งาน สําคัญ ใน วิสัยทัศน์ ทาง คอมพิวเตอร์ ซึ่ง มุ่ง ที่จะ ระบุ และ ระบุ วัตถุ ต่าง\n",
            "ๆ\n",
            "ในการ ระบุ และ ระบุ ปัญหา ใน คอมพิวเตอร์ หรือ การ ตรวจจับ วัตถุ ที่เกิดขึ้น\n",
            "ความก้าวหน้า ล่าสุด ใน การเรียนรู้ ลึก และ เครือข่าย\n",
            "Ne ur al\n",
            "In te mp ation\n",
            "Network s\n",
            "ได้ ปรับปรุง ประสิทธิภาพ ของ การตรวจสอบ วัตถุ\n",
            "เครือข่าย\n",
            "Ne ur al\n",
            "TM\n",
            "ได้ ปรับปรุง ประสิทธิภาพ ของ เทคนิค การ ตรวจจับ วัตถุ ใน\n",
            "W e ff ect\n",
            "TM\n",
            "ซึ่ง รวมถึง เทคนิค การ ตรวจจับ วัตถุ หลาย\n",
            "ๆ\n",
            "อย่าง ใน\n",
            "S n ow f or r ame s\n",
            "ต่าง\n",
            "ๆ\n",
            "ที่ ได้ แสดง ผล การวิเคราะห์ เปรียบเทียบ ของ วิธีการ และ เน้น จุด แข็ง และ จุด โห ว ่ และ จุด โห ว ่ ของ วัตถุ ต่าง\n",
            "ๆ\n",
            "เรา นําเสนอ ทิศทาง การวิจัย ในอนาคต เพื่อ ปรับปรุง การ ตรวจจับ วัตถุ ในอนาคต ใน\n",
            "S an\n",
            "An ton io\n",
            "An ton io\n",
            "ได้ ปรับตัว ขึ้น จริง\n",
            "มุมมอง ของ วัตถุ\n",
            "ne\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tf74fGcyFF3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eEd6y4rNFF1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_sentence = 'The principal author of the Declaration of Independence, Jefferson was a proponent of democracy, republicanism, and individual rights, motivating American colonists to break from the Kingdom of Great Britain and form a new nation'\n",
        "# tokenized_sentence = ' '.join(en_word_tokenize(input_sentence))\n",
        "\n",
        "# print('input_sentence:', input_sentence)\n",
        "# print('tokenized_sentence:', tokenized_sentence)"
      ],
      "metadata": {
        "id": "OUKa_kLjz7BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hypothesis = en2th_word2bpe.translate(tokenized_sentence)\n",
        "\n",
        "# print('hypothesis: ', hypothesis)\n",
        "# hypothesis = hypothesis.replace(' ', '').replace('▁', ' ').strip()\n",
        "# print('\\ntranslation:', hypothesis)"
      ],
      "metadata": {
        "id": "52QDWLlPz_Qr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}