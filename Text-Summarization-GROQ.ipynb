{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout", 
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: C:\\Users\\Nitro 5\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from groq) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from groq) (2.7.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.5.0->groq) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\nitro 5\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->groq) (2.18.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: C:\\Users\\Nitro 5\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = \"API-KEY\"\n",
    "client = Groq(api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_words):\n",
    "    prompt = f\"Summarize the following text in approximately {max_words} words:\\n\\n{text}\\n\\nSummary:\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides concise summaries.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def summarize_chunks(chunks, max_words_per_chunk=100, max_words_final=500):\n",
    "    chunk_summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summarize_text(chunk.page_content, max_words_per_chunk)\n",
    "        chunk_summaries.append(summary)\n",
    "    combined_summaries = \"\\n\\n\".join(chunk_summaries)\n",
    "    final_summary = summarize_text(combined_summaries, max_words_final)\n",
    "    return final_summary\n",
    "\n",
    "# chat = abstractive_summarize(\"Explain the importance of low latency LLMs\")\n",
    "# print(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf\"\n",
    "\n",
    "# Local PDF file uploads\n",
    "if filepath:\n",
    "  loader = UnstructuredPDFLoader(file_path=filepath)\n",
    "  data = loader.load()\n",
    "else:\n",
    "  print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Comprehensive Study on Object Detection Techniques in Unconstrained Environments\n",
      "\n",
      "Hrishitva Patel1\n",
      "\n",
      "1The University of Texas at San Antonio hrishitva.patel@my.utsa.edu\n",
      "\n",
      "Abstract Object detection is a crucial task in computer vision that aims to identify and localize objects in images or videos. The recent advancements in deep learning and Convolutional Neural Networks (CNNs) have significantly improved the performance of object detection techniques. This paper presents a comprehensive study of object detection techniques in unconstrained environments, including various challenges, datasets, and state-of-the-art approaches. Additionally, we present a comparative analysis of the methods and highlight their strengths and weaknesses. Finally, we provide some future research directions to further improve object detection in unconstrained environments.\n",
      "\n",
      "Keywords: object detection, unconstrained environments, deep learning, convolutional neural networks, computer vision\n",
      "\n",
      "1 Introduction and Background\n",
      "\n",
      "Object detection is a fundamental problem in computer vision, with numerous applications spanning fields such as surveillance, robotics, autonomous vehicles, augmented reality, and human-computer interaction. The primary goal of object detection is to recognize and localize instances of objects belonging to predefined classes in images or videos. In recent years, significant progress has been made in the development of object detection algorithms, mainly due to the emergence of deep learning and Convolutional Neural Networks (CNNs). These advancements have led to impressive performance improvements in various benchmark datasets, such as PASCAL VOC, ImageNet, and MS COCO. Despite these successes, object detection in unconstrained environments remains a challenging task. Unconstrained environments are characterized by variations in lighting conditions, viewpoint changes, occlusions, object deformations, scale changes, and the presence of cluttered backgrounds. These factors can severely affect the performance of object detection algorithms, making it difficult to achieve high detection accuracy and robustness.\n",
      "\n",
      "In recent years, significant progress has been made in object detection, particularly in the area of deep learning and Convolutional Neural Networks (CNNs) [1]. These techniques have significantly improved the performance of object detection algorithms, particularly in unconstrained environments where objects may appear at different scales, angles, and orientations. Region-based object detectors, such as Region-based Convolutional Neural Networks (R-CNN) [2], operate by first generating region proposals using a selective search algorithm, which\n",
      "\n",
      "generates around 2000 regions per image. Each region is then passed through a CNN to generate a fixed-length feature vector, which is fed into a support vector machine (SVM) [3] to classify the region and predict its bounding box coordinates. Finally, non-maximum suppression is applied to eliminate redundant detections. While R-CNN was a significant breakthrough in object detection, it has several limitations, such as slow training and inference times.\n",
      "\n",
      "To address these issues, researchers have proposed several variants of R-CNN, such as Fast R- CNN [4], which shares convolutional features across region proposals, and Faster R-CNN , which introduces a Region Proposal Network (RPN) to generate region proposals in an end-to-end manner. These variants significantly improve the speed and accuracy of R-CNN, making it a popular choice for object detection in unconstrained environments. The purpose of this paper is to provide a comprehensive overview of object detection techniques in unconstrained environments, addressing the challenges, datasets, and state-of-the-art approaches.\n",
      "\n",
      "The paper is organized as follows: Section 2 discusses the challenges encountered in object detection in unconstrained environments, highlighting the factors that contribute to the complexity of the problem. Section 3 presents a review of the commonly used datasets for evaluating object detection techniques in unconstrained environments. Section 4 presents state of the art Objection detection techniques. Section 5 presents a comparative analysis of the surveyed methods, emphasizing their strengths and weaknesses in terms of accuracy, computational complexity, and robustness to variations in the unconstrained environment. Section 6 concludes the paper by highlighting some of the open research questions and future directions in the field of object detection in unconstrained environments.\n",
      "\n",
      "2 Challenges in Object Detection in Unconstrained Environments 2.1 Illumination Changes\n",
      "\n",
      "Variations in lighting conditions, such as shadows and overexposure, can significantly impact the appearance of objects, making it difficult for detection algorithms to identify and localize them accurately. Variations in lighting conditions, such as shadows, overexposure, or underexposure, can significantly impact the appearance of objects in images [5]. These changes can make it difficult for detection algorithms to identify and localize objects accurately. To address this issue, several approaches have been proposed, including color constancy techniques [6] and deep learning-based methods that can learn illumination invariant features [7].\n",
      "\n",
      "2.2 Viewpoint Variation\n",
      "\n",
      "Changes in the viewpoint or camera angle can alter the object's appearance, causing the detection algorithm to fail in recognizing the object or produce inaccurate bounding boxes [8]. Several methods have been proposed to tackle this issue, such as viewpoint invariant features and multi- view object detectors [8].\n",
      "\n",
      "2.3 Occlusion\n",
      "\n",
      "Objects in the scene may be partially or entirely occluded by other objects, making it challenging for the detection algorithm to identify and localize them correctly [9]. To address occlusion, some methods employ part-based models [10] or leverage context information from surrounding regions.\n",
      "\n",
      "3 Datasets\n",
      "\n",
      "Object detection is a vital task in computer vision that involves identifying the presence and location of objects in an image or video. To evaluate the performance of object detection techniques in unconstrained environments, several benchmark datasets have been created. These datasets provide a standardized set of images with labeled objects, enabling researchers to compare the accuracy and speed of different algorithms. Some popular datasets include:\n",
      "\n",
      "3.1 Pascal VOC\n",
      "\n",
      "The PASCAL VOC (Visual Object Classes) dataset is one of the oldest and most popular datasets for object detection. It contains 17,125 images with 20 object classes, such as person, car, and dog. The dataset provides bounding box annotations for each object in the image. PASCAL VOC has been used as a benchmark dataset for several years, and many state-of-the-art object detection techniques have been evaluated on this dataset.\n",
      "\n",
      "3.2 ImageNet\n",
      "\n",
      "The ImageNet dataset is a massive dataset that contains 1.2 million images with 1,000 object classes. Unlike PASCAL VOC, ImageNet does not provide annotations for object detection. However, many researchers have used this dataset to pre-train their models on a large amount of data before fine-tuning them on smaller object detection datasets.\n",
      "\n",
      "3.3 COCO\n",
      "\n",
      "The COCO (Common Objects in Context) dataset is a newer dataset that contains 330,000 images with 80 object classes. COCO provides more detailed annotations than PASCAL VOC, including segmentation masks for each object in the image. This makes COCO a more challenging dataset for object detection algorithms to perform well on.\n",
      "\n",
      "3.4 Open Images\n",
      "\n",
      "The Open Images dataset is another large-scale dataset that contains 1.7 million images with 600 object classes. It provides both bounding box and segmentation mask annotations and has been used as a benchmark for object detection algorithms that require large amounts of training data.\n",
      "\n",
      "These datasets vary in size, number of classes, and annotation types, allowing researchers to test their algorithms on a wide range of scenarios. The following table summarizes some key information about the four popular benchmark datasets used for evaluating object detection techniques:\n",
      "\n",
      "Table 1. Summary of key information about benchmark datasets for object detection\n",
      "\n",
      "Dataset Name\n",
      "\n",
      "Number of Images\n",
      "\n",
      "Number of Classes\n",
      "\n",
      "Annotation Type\n",
      "\n",
      "PASCAL VOC[11]\n",
      "\n",
      "17,125\n",
      "\n",
      "20\n",
      "\n",
      "Bounding Boxes\n",
      "\n",
      "ImageNet [12]\n",
      "\n",
      "1.2 million\n",
      "\n",
      "1,000\n",
      "\n",
      "Bounding Boxes\n",
      "\n",
      "COCO [13]\n",
      "\n",
      "330,000\n",
      "\n",
      "80\n",
      "\n",
      "Bounding Boxes\n",
      "\n",
      "Open Images [14]\n",
      "\n",
      "1.7 million\n",
      "\n",
      "600\n",
      "\n",
      "Mask RCNN\n",
      "\n",
      "4 State-of-the-art Object Detection Techniques\n",
      "\n",
      "We categorize the state-of-the-art object detection techniques into two main groups: two-stage detectors and single-stage detectors.\n",
      "\n",
      "Figure 1. Milestones of object detection [15].\n",
      "\n",
      "4.1 Two-stage detectors\n",
      "\n",
      "Two-stage detectors consist of a region proposal stage followed by a classification stage. Some prominent two-stage detectors include:\n",
      "\n",
      "4.1.1 R-CNN\n",
      "\n",
      "R-CNN (Region-based Convolutional Neural Networks) is an object detection model that was proposed in 2014 by Ross Girshick et al. R-CNN is a two-stage object detection framework that uses a region proposal mechanism to generate potential object regions in an image and then applies a convolutional neural network (CNN) to classify and refine these regions.\n",
      "\n",
      "The R-CNN framework consists of the following steps:\n",
      "\n",
      "1. Region Proposal: The first stage of R-CNN generates potential object regions by using a selective search algorithm that combines low-level features, such as color and texture, with high-level cues, such as edges and corners. Selective search generates around 2,000 region proposals for each image.\n",
      "\n",
      "2. Feature Extraction: In the second stage, each region proposal is warped to a fixed size and fed through a pre-trained CNN, such as Alex Net or VGG, to extract a feature vector for that region.\n",
      "\n",
      "3. Object Classification and Refinement: The feature vector for each region proposal is then fed into a set of fully connected layers that perform object classification and bounding box regression. The classification layer outputs the probability of each region proposal containing a particular object class, while the regression layer outputs the refined bounding box coordinates for that object class.\n",
      "\n",
      "4.1.2 Fast R-CNN\n",
      "\n",
      "Faster R-CNN (Region-based Convolutional Neural Networks): Faster R-CNN is a two-stage object detection model that uses a Region Proposal Network (RPN) to generate object proposals and a Fast R-CNN network to classify and refine the proposals. The RPN generates region proposals by sliding a small network over the convolutional feature map and predicting abjectness scores and bounding box offsets. Faster R-CNN is known for its accuracy and has been widely used in object detection tasks.\n",
      "\n",
      "4.2 Single-stage detectors\n",
      "\n",
      "Single-stage detectors directly predict object bounding boxes and class probabilities from an image. Some popular single-stage detectors include:\n",
      "\n",
      "4.2.1 YOLO\n",
      "\n",
      "YOLO is another one-stage object detection model that predicts object class scores and bounding box offsets directly from the entire image. YOLO divides the image into a grid of cells and predicts the class and bounding box for each cell. YOLO uses a single neural network to make predictions and is known for its speed and real-time performance.\n",
      "\n",
      "4.2.2 SSD\n",
      "\n",
      "The Single Shot MultiBox Detector (SSD) extends the concept of YOLO by predicting bounding boxes and class probabilities at multiple scales, which improves the detection of objects with varying sizes. SSD uses a feature extractor to generate convolutional feature maps and applies a set of convolutional filters to predict class scores and offsets for each default box. SSD is known for its speed and efficiency and has been used in real-time object detection applications.\n",
      "\n",
      "4.2.3 Retina Net\n",
      "\n",
      "Retina Net introduces the Focal Loss, which addresses the issue of class imbalance by down weighting the contribution of easy examples and focusing on hard examples during training. This results in improved detection performance, particularly for small objects. Retina Net uses a novel focal loss function that assigns higher weights to hard examples and reduces the effect of easy examples during training. Retina Net also uses a Feature Pyramid Network (FPN) to handle objects at different scales and has achieved state-of-the-art performance on several object detection benchmarks.\n",
      "\n",
      "Figure 2. One stage vs two stage object detection.\n",
      "\n",
      "The below table summarizes some key features of these state-of-the-art object detection techniques:\n",
      "\n",
      "Table 2. Key features of state-of-the-art object detection techniques\n",
      "\n",
      "Technique\n",
      "\n",
      "Training Time Inference Time Number of Parameters AP on COCO\n",
      "\n",
      "Faster R-CNN [16] Long\n",
      "\n",
      "Medium\n",
      "\n",
      "High\n",
      "\n",
      "39.3\n",
      "\n",
      "SSD [17]\n",
      "\n",
      "Medium\n",
      "\n",
      "Fast\n",
      "\n",
      "Low\n",
      "\n",
      "31.2\n",
      "\n",
      "YOLO [18]\n",
      "\n",
      "Short\n",
      "\n",
      "Very Fast\n",
      "\n",
      "Low\n",
      "\n",
      "28.2\n",
      "\n",
      "Retina Net [19]\n",
      "\n",
      "Long\n",
      "\n",
      "Medium\n",
      "\n",
      "High\n",
      "\n",
      "39.1\n",
      "\n",
      "5 Comparative Analysis\n",
      "\n",
      "In this section, we compare the performance of various object detection techniques on the COCO dataset [5]. The results are summarized in Table 1.\n",
      "\n",
      "Table 1: Comparison of object detection techniques on the COCO dataset\n",
      "\n",
      "Method\n",
      "\n",
      "Average Precision (AP)\n",
      "\n",
      "Speed (fps)\n",
      "\n",
      "R-CNN\n",
      "\n",
      "53.3\n",
      "\n",
      "0.5\n",
      "\n",
      "Fast R-CNN\n",
      "\n",
      "70.0\n",
      "\n",
      "5\n",
      "\n",
      "Faster R-CNN\n",
      "\n",
      "73.2\n",
      "\n",
      "7\n",
      "\n",
      "YOLOv3\n",
      "\n",
      "57.9\n",
      "\n",
      "45\n",
      "\n",
      "SSD\n",
      "\n",
      "72.1\n",
      "\n",
      "19\n",
      "\n",
      "RetinaNet\n",
      "\n",
      "74.8\n",
      "\n",
      "12\n",
      "\n",
      "The results in Table 1 show that two-stage detectors, such as Faster R-CNN, generally achieve higher average precision (AP) compared to single-stage detectors like YOLOv3 and SSD. However, single-stage detectors are faster in terms of frames per second (fps), making them more suitable for real-time applications.\n",
      "\n",
      "Figure 3. Comparison of object detection techniques on the COCO dataset\n",
      "\n",
      "6 Conclusion and Future Directions In this paper, we have presented a comprehensive study on object detection techniques in unconstrained environments. We have discussed the challenges associated with object detection\n",
      "\n",
      "in such environments, presented popular datasets, and provided an overview of the state-of-the-art techniques. Additionally, we have compared the performance of various methods and highlighted their strengths and weaknesses.\n",
      "\n",
      "Despite the significant progress made in recent years, object detection in unconstrained environments remains a challenging problem. Future research directions could focus on the following aspects:\n",
      "\n",
      "Developing more robust algorithms capable of handling occlusions, lighting variations, and background clutter. Investigating techniques for efficient and accurate detection of small-scale objects.\n",
      "\n",
      "• Exploring the integration of other sensor modalities, such as LiDAR or depth information, to enhance object detection performance.\n",
      "\n",
      "Developing unsupervised or weakly supervised object detection techniques to reduce the reliance on large-scale annotated datasets.\n",
      "\n",
      "By addressing these challenges and exploring new approaches, we believe that object detection in unconstrained environments can be further improved, paving the way for more reliable and efficient applications in various domains, such as autonomous vehicles, robotics, and surveillance systems.\n",
      "\n",
      "References\n",
      "\n",
      "1.\n",
      "\n",
      "2.\n",
      "\n",
      "3.\n",
      "\n",
      "4.\n",
      "\n",
      "5.\n",
      "\n",
      "6.\n",
      "\n",
      "7.\n",
      "\n",
      "8.\n",
      "\n",
      "9.\n",
      "\n",
      "10.\n",
      "\n",
      "Dhillon, A. and G.K.J.P.i.A.I. Verma, Convolutional neural network: a review of models, methodologies and applications to object detection. 2020. 9(2): p. 85-112. Xu, C., et al., A page object detection method based on mask R-CNN. 2021. 9: p. 143448- 143457. Qin, W., R. Elanwar, and M.J.J.o.I.S. Betke, Text and metadata extraction from scanned Arabic documents using support vector machines. 2022. 48(2): p. 268-279. Liu, B., W. Zhao, and Q. Sun. Study of object detection based on Faster R-CNN. in 2017 Chinese Automation Congress (CAC). 2017. IEEE. Wong, J.K.W., et al., Development of a refined illumination and reflectance approach for optimal construction site interior image enhancement. 2022. Li, Y., et al., A unified probabilistic framework of robust and efficient color consistency correction for multiple images. 2022. 190: p. 1-24. Csurka, G. and M.J.a.p.a. Humenberger, From handcrafted to deep local invariant features. 2018. 2: p. 1. Doi, K., et al., Detecting Object-Level Scene Changes in Images with Viewpoint Differences Using Graph Matching. 2022. 14(17): p. 4225. Cao, Z., et al., A Multi-Object Tracking Algorithm With Center-Based Feature Extraction and Occlusion Handling. 2022. Somers, V., C. De Vleeschouwer, and A. Alahi. Body Part-Based Representation Learning for Occluded Person Re-Identification. in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023. Hwang, B., S. Lee, and H.J.E. Han, LNFCOS: Efficient Object Detection through Deep Learning Based on LNblock. 2022. 11(17): p. 2783.\n",
      "\n",
      "11.\n",
      "\n",
      "12.\n",
      "\n",
      "Zhao, J., et al., Data-adaptive binary neural networks for efficient object detection and recognition. 2022. 153: p. 239-245.\n",
      "\n",
      "13. Ma, J., Y. Ushiku, and M. Sagara. The effect of improving annotation quality on object detection datasets: A preliminary study. in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. Kuznetsova, A., et al., The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. 2020. 128(7): p. 1956-1981. Xiao, Y., et al., A review of object detection based on deep learning. 2020. 79: p. 23729- 23791. Rani, S., et al., Object detection and recognition using contour based edge detection and fast R-CNN. 2022. 81(29): p. 42183-42207. Feroz, M.A., et al. Object detection and classification from a real-time video using SSD and YOLO models. in Computational Intelligence in Pattern Recognition: Proceedings of CIPR 2021. 2022. Springer. Diwan, T., et al., Object detection using YOLO: Challenges, architectural successors, datasets and applications. 2022: p. 1-33. Li, G., et al. Knowledge distillation for object detection via rank mimicking and prediction- guided feature imitation. in Proceedings of the AAAI Conference on Artificial Intelligence. 2022. 19.\n",
      "\n",
      "14.\n",
      "\n",
      "15.\n",
      "\n",
      "16.\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 17939\n"
     ]
    }
   ],
   "source": [
    "total_characters = 0\n",
    "\n",
    "for page in data:\n",
    "    page_content = page.page_content\n",
    "    total_characters += len(page_content)\n",
    "\n",
    "print(f\"Total number of characters: {total_characters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 3062\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "\n",
    "def count_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "for page in data:\n",
    "    page_content = page.page_content\n",
    "    page_word_count = count_words(page_content)\n",
    "    total_words += page_word_count\n",
    "\n",
    "print(f\"Total number of words: {total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A Comprehensive Study on Object Detection Techniques in Unconstrained Environments\\n\\nHrishitva Patel1\\n\\n1The University of Texas at San Antonio hrishitva.patel@my.utsa.edu\\n\\nAbstract Object detection is a crucial task in computer vision that aims to identify and localize objects in images or videos. The recent advancements in deep learning and Convolutional Neural Networks (CNNs) have significantly improved the performance of object detection techniques. This paper presents a comprehensive study of object detection techniques in unconstrained environments, including various challenges, datasets, and state-of-the-art approaches. Additionally, we present a comparative analysis of the methods and highlight their strengths and weaknesses. Finally, we provide some future research directions to further improve object detection in unconstrained environments.\\n\\nKeywords: object detection, unconstrained environments, deep learning, convolutional neural networks, computer vision\\n\\n1 Introduction and Background', metadata={'source': 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf'}),\n",
       " Document(page_content='1 Introduction and Background\\n\\nObject detection is a fundamental problem in computer vision, with numerous applications spanning fields such as surveillance, robotics, autonomous vehicles, augmented reality, and human-computer interaction. The primary goal of object detection is to recognize and localize instances of objects belonging to predefined classes in images or videos. In recent years, significant progress has been made in the development of object detection algorithms, mainly due to the emergence of deep learning and Convolutional Neural Networks (CNNs). These advancements have led to impressive performance improvements in various benchmark datasets, such as PASCAL VOC, ImageNet, and MS COCO. Despite these successes, object detection in unconstrained environments remains a challenging task. Unconstrained environments are characterized by variations in lighting conditions, viewpoint changes, occlusions, object deformations, scale changes, and the presence of cluttered backgrounds. These factors can severely affect the performance of object detection algorithms, making it difficult to achieve high detection accuracy and robustness.\\n\\nIn recent years, significant progress has been made in object detection, particularly in the area of deep learning and Convolutional Neural Networks (CNNs) [1]. These techniques have significantly improved the performance of object detection algorithms, particularly in unconstrained environments where objects may appear at different scales, angles, and orientations. Region-based object detectors, such as Region-based Convolutional Neural Networks (R-CNN) [2], operate by first generating region proposals using a selective search algorithm, which', metadata={'source': 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf'}),\n",
       " Document(page_content='generates around 2000 regions per image. Each region is then passed through a CNN to generate a fixed-length feature vector, which is fed into a support vector machine (SVM) [3] to classify the region and predict its bounding box coordinates. Finally, non-maximum suppression is applied to eliminate redundant detections. While R-CNN was a significant breakthrough in object detection, it has several limitations, such as slow training and inference times.\\n\\nTo address these issues, researchers have proposed several variants of R-CNN, such as Fast R- CNN [4], which shares convolutional features across region proposals, and Faster R-CNN , which introduces a Region Proposal Network (RPN) to generate region proposals in an end-to-end manner. These variants significantly improve the speed and accuracy of R-CNN, making it a popular choice for object detection in unconstrained environments. The purpose of this paper is to provide a comprehensive overview of object detection techniques in unconstrained environments, addressing the challenges, datasets, and state-of-the-art approaches.\\n\\nThe paper is organized as follows: Section 2 discusses the challenges encountered in object detection in unconstrained environments, highlighting the factors that contribute to the complexity of the problem. Section 3 presents a review of the commonly used datasets for evaluating object detection techniques in unconstrained environments. Section 4 presents state of the art Objection detection techniques. Section 5 presents a comparative analysis of the surveyed methods, emphasizing their strengths and weaknesses in terms of accuracy, computational complexity, and robustness to variations in the unconstrained environment. Section 6 concludes the paper by highlighting some of the open research questions and future directions in the field of object detection in unconstrained environments.\\n\\n2 Challenges in Object Detection in Unconstrained Environments 2.1 Illumination Changes', metadata={'source': 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf'}),\n",
       " Document(page_content=\"2 Challenges in Object Detection in Unconstrained Environments 2.1 Illumination Changes\\n\\nVariations in lighting conditions, such as shadows and overexposure, can significantly impact the appearance of objects, making it difficult for detection algorithms to identify and localize them accurately. Variations in lighting conditions, such as shadows, overexposure, or underexposure, can significantly impact the appearance of objects in images [5]. These changes can make it difficult for detection algorithms to identify and localize objects accurately. To address this issue, several approaches have been proposed, including color constancy techniques [6] and deep learning-based methods that can learn illumination invariant features [7].\\n\\n2.2 Viewpoint Variation\\n\\nChanges in the viewpoint or camera angle can alter the object's appearance, causing the detection algorithm to fail in recognizing the object or produce inaccurate bounding boxes [8]. Several methods have been proposed to tackle this issue, such as viewpoint invariant features and multi- view object detectors [8].\\n\\n2.3 Occlusion\\n\\nObjects in the scene may be partially or entirely occluded by other objects, making it challenging for the detection algorithm to identify and localize them correctly [9]. To address occlusion, some methods employ part-based models [10] or leverage context information from surrounding regions.\\n\\n3 Datasets\\n\\nObject detection is a vital task in computer vision that involves identifying the presence and location of objects in an image or video. To evaluate the performance of object detection techniques in unconstrained environments, several benchmark datasets have been created. These datasets provide a standardized set of images with labeled objects, enabling researchers to compare the accuracy and speed of different algorithms. Some popular datasets include:\\n\\n3.1 Pascal VOC\", metadata={'source': 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf'}),\n",
       " Document(page_content='3.1 Pascal VOC\\n\\nThe PASCAL VOC (Visual Object Classes) dataset is one of the oldest and most popular datasets for object detection. It contains 17,125 images with 20 object classes, such as person, car, and dog. The dataset provides bounding box annotations for each object in the image. PASCAL VOC has been used as a benchmark dataset for several years, and many state-of-the-art object detection techniques have been evaluated on this dataset.\\n\\n3.2 ImageNet\\n\\nThe ImageNet dataset is a massive dataset that contains 1.2 million images with 1,000 object classes. Unlike PASCAL VOC, ImageNet does not provide annotations for object detection. However, many researchers have used this dataset to pre-train their models on a large amount of data before fine-tuning them on smaller object detection datasets.\\n\\n3.3 COCO\\n\\nThe COCO (Common Objects in Context) dataset is a newer dataset that contains 330,000 images with 80 object classes. COCO provides more detailed annotations than PASCAL VOC, including segmentation masks for each object in the image. This makes COCO a more challenging dataset for object detection algorithms to perform well on.\\n\\n3.4 Open Images\\n\\nThe Open Images dataset is another large-scale dataset that contains 1.7 million images with 600 object classes. It provides both bounding box and segmentation mask annotations and has been used as a benchmark for object detection algorithms that require large amounts of training data.\\n\\nThese datasets vary in size, number of classes, and annotation types, allowing researchers to test their algorithms on a wide range of scenarios. The following table summarizes some key information about the four popular benchmark datasets used for evaluating object detection techniques:\\n\\nTable 1. Summary of key information about benchmark datasets for object detection\\n\\nDataset Name\\n\\nNumber of Images\\n\\nNumber of Classes\\n\\nAnnotation Type\\n\\nPASCAL VOC[11]\\n\\n17,125\\n\\n20\\n\\nBounding Boxes\\n\\nImageNet [12]\\n\\n1.2 million\\n\\n1,000\\n\\nBounding Boxes\\n\\nCOCO [13]', metadata={'source': 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf'}),\n",
       " Document(page_content='17,125\\n\\n20\\n\\nBounding Boxes\\n\\nImageNet [12]\\n\\n1.2 million\\n\\n1,000\\n\\nBounding Boxes\\n\\nCOCO [13]\\n\\n330,000\\n\\n80\\n\\nBounding Boxes\\n\\nOpen Images [14]\\n\\n1.7 million\\n\\n600\\n\\nMask RCNN\\n\\n4 State-of-the-art Object Detection Techniques\\n\\nWe categorize the state-of-the-art object detection techniques into two main groups: two-stage detectors and single-stage detectors.\\n\\nFigure 1. Milestones of object detection [15].\\n\\n4.1 Two-stage detectors\\n\\nTwo-stage detectors consist of a region proposal stage followed by a classification stage. Some prominent two-stage detectors include:\\n\\n4.1.1 R-CNN\\n\\nR-CNN (Region-based Convolutional Neural Networks) is an object detection model that was proposed in 2014 by Ross Girshick et al. R-CNN is a two-stage object detection framework that uses a region proposal mechanism to generate potential object regions in an image and then applies a convolutional neural network (CNN) to classify and refine these regions.\\n\\nThe R-CNN framework consists of the following steps:\\n\\n1. Region Proposal: The first stage of R-CNN generates potential object regions by using a selective search algorithm that combines low-level features, such as color and texture, with high-level cues, such as edges and corners. Selective search generates around 2,000 region proposals for each image.\\n\\n2. Feature Extraction: In the second stage, each region proposal is warped to a fixed size and fed through a pre-trained CNN, such as Alex Net or VGG, to extract a feature vector for that region.\\n\\n3. Object Classification and Refinement: The feature vector for each region proposal is then fed into a set of fully connected layers that perform object classification and bounding box regression. The classification layer outputs the probability of each region proposal containing a particular object class, while the regression layer outputs the refined bounding box coordinates for that object class.\\n\\n4.1.2 Fast R-CNN', metadata={'source': 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf'}),\n",
       " Document(page_content='4.1.2 Fast R-CNN\\n\\nFaster R-CNN (Region-based Convolutional Neural Networks): Faster R-CNN is a two-stage object detection model that uses a Region Proposal Network (RPN) to generate object proposals and a Fast R-CNN network to classify and refine the proposals. The RPN generates region proposals by sliding a small network over the convolutional feature map and predicting abjectness scores and bounding box offsets. Faster R-CNN is known for its accuracy and has been widely used in object detection tasks.\\n\\n4.2 Single-stage detectors\\n\\nSingle-stage detectors directly predict object bounding boxes and class probabilities from an image. Some popular single-stage detectors include:\\n\\n4.2.1 YOLO\\n\\nYOLO is another one-stage object detection model that predicts object class scores and bounding box offsets directly from the entire image. YOLO divides the image into a grid of cells and predicts the class and bounding box for each cell. YOLO uses a single neural network to make predictions and is known for its speed and real-time performance.\\n\\n4.2.2 SSD\\n\\nThe Single Shot MultiBox Detector (SSD) extends the concept of YOLO by predicting bounding boxes and class probabilities at multiple scales, which improves the detection of objects with varying sizes. SSD uses a feature extractor to generate convolutional feature maps and applies a set of convolutional filters to predict class scores and offsets for each default box. SSD is known for its speed and efficiency and has been used in real-time object detection applications.\\n\\n4.2.3 Retina Net', metadata={'source': 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf'}),\n",
       " Document(page_content='4.2.3 Retina Net\\n\\nRetina Net introduces the Focal Loss, which addresses the issue of class imbalance by down weighting the contribution of easy examples and focusing on hard examples during training. This results in improved detection performance, particularly for small objects. Retina Net uses a novel focal loss function that assigns higher weights to hard examples and reduces the effect of easy examples during training. Retina Net also uses a Feature Pyramid Network (FPN) to handle objects at different scales and has achieved state-of-the-art performance on several object detection benchmarks.\\n\\nFigure 2. One stage vs two stage object detection.\\n\\nThe below table summarizes some key features of these state-of-the-art object detection techniques:\\n\\nTable 2. Key features of state-of-the-art object detection techniques\\n\\nTechnique\\n\\nTraining Time Inference Time Number of Parameters AP on COCO\\n\\nFaster R-CNN [16] Long\\n\\nMedium\\n\\nHigh\\n\\n39.3\\n\\nSSD [17]\\n\\nMedium\\n\\nFast\\n\\nLow\\n\\n31.2\\n\\nYOLO [18]\\n\\nShort\\n\\nVery Fast\\n\\nLow\\n\\n28.2\\n\\nRetina Net [19]\\n\\nLong\\n\\nMedium\\n\\nHigh\\n\\n39.1\\n\\n5 Comparative Analysis\\n\\nIn this section, we compare the performance of various object detection techniques on the COCO dataset [5]. The results are summarized in Table 1.\\n\\nTable 1: Comparison of object detection techniques on the COCO dataset\\n\\nMethod\\n\\nAverage Precision (AP)\\n\\nSpeed (fps)\\n\\nR-CNN\\n\\n53.3\\n\\n0.5\\n\\nFast R-CNN\\n\\n70.0\\n\\n5\\n\\nFaster R-CNN\\n\\n73.2\\n\\n7\\n\\nYOLOv3\\n\\n57.9\\n\\n45\\n\\nSSD\\n\\n72.1\\n\\n19\\n\\nRetinaNet\\n\\n74.8\\n\\n12\\n\\nThe results in Table 1 show that two-stage detectors, such as Faster R-CNN, generally achieve higher average precision (AP) compared to single-stage detectors like YOLOv3 and SSD. However, single-stage detectors are faster in terms of frames per second (fps), making them more suitable for real-time applications.\\n\\nFigure 3. Comparison of object detection techniques on the COCO dataset', metadata={'source': 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf'}),\n",
       " Document(page_content='Figure 3. Comparison of object detection techniques on the COCO dataset\\n\\n6 Conclusion and Future Directions In this paper, we have presented a comprehensive study on object detection techniques in unconstrained environments. We have discussed the challenges associated with object detection\\n\\nin such environments, presented popular datasets, and provided an overview of the state-of-the-art techniques. Additionally, we have compared the performance of various methods and highlighted their strengths and weaknesses.\\n\\nDespite the significant progress made in recent years, object detection in unconstrained environments remains a challenging problem. Future research directions could focus on the following aspects:\\n\\nDeveloping more robust algorithms capable of handling occlusions, lighting variations, and background clutter. Investigating techniques for efficient and accurate detection of small-scale objects.\\n\\n• Exploring the integration of other sensor modalities, such as LiDAR or depth information, to enhance object detection performance.\\n\\nDeveloping unsupervised or weakly supervised object detection techniques to reduce the reliance on large-scale annotated datasets.\\n\\nBy addressing these challenges and exploring new approaches, we believe that object detection in unconstrained environments can be further improved, paving the way for more reliable and efficient applications in various domains, such as autonomous vehicles, robotics, and surveillance systems.\\n\\nReferences\\n\\n1.\\n\\n2.\\n\\n3.\\n\\n4.\\n\\n5.\\n\\n6.\\n\\n7.\\n\\n8.\\n\\n9.\\n\\n10.', metadata={'source': 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf'}),\n",
       " Document(page_content='References\\n\\n1.\\n\\n2.\\n\\n3.\\n\\n4.\\n\\n5.\\n\\n6.\\n\\n7.\\n\\n8.\\n\\n9.\\n\\n10.\\n\\nDhillon, A. and G.K.J.P.i.A.I. Verma, Convolutional neural network: a review of models, methodologies and applications to object detection. 2020. 9(2): p. 85-112. Xu, C., et al., A page object detection method based on mask R-CNN. 2021. 9: p. 143448- 143457. Qin, W., R. Elanwar, and M.J.J.o.I.S. Betke, Text and metadata extraction from scanned Arabic documents using support vector machines. 2022. 48(2): p. 268-279. Liu, B., W. Zhao, and Q. Sun. Study of object detection based on Faster R-CNN. in 2017 Chinese Automation Congress (CAC). 2017. IEEE. Wong, J.K.W., et al., Development of a refined illumination and reflectance approach for optimal construction site interior image enhancement. 2022. Li, Y., et al., A unified probabilistic framework of robust and efficient color consistency correction for multiple images. 2022. 190: p. 1-24. Csurka, G. and M.J.a.p.a. Humenberger, From handcrafted to deep local invariant features. 2018. 2: p. 1. Doi, K., et al., Detecting Object-Level Scene Changes in Images with Viewpoint Differences Using Graph Matching. 2022. 14(17): p. 4225. Cao, Z., et al., A Multi-Object Tracking Algorithm With Center-Based Feature Extraction and Occlusion Handling. 2022. Somers, V., C. De Vleeschouwer, and A. Alahi. Body Part-Based Representation Learning for Occluded Person Re-Identification. in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023. Hwang, B., S. Lee, and H.J.E. Han, LNFCOS: Efficient Object Detection through Deep Learning Based on LNblock. 2022. 11(17): p. 2783.\\n\\n11.\\n\\n12.\\n\\nZhao, J., et al., Data-adaptive binary neural networks for efficient object detection and recognition. 2022. 153: p. 239-245.', metadata={'source': 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf'}),\n",
       " Document(page_content='13. Ma, J., Y. Ushiku, and M. Sagara. The effect of improving annotation quality on object detection datasets: A preliminary study. in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. Kuznetsova, A., et al., The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. 2020. 128(7): p. 1956-1981. Xiao, Y., et al., A review of object detection based on deep learning. 2020. 79: p. 23729- 23791. Rani, S., et al., Object detection and recognition using contour based edge detection and fast R-CNN. 2022. 81(29): p. 42183-42207. Feroz, M.A., et al. Object detection and classification from a real-time video using SSD and YOLO models. in Computational Intelligence in Pattern Recognition: Proceedings of CIPR 2021. 2022. Springer. Diwan, T., et al., Object detection using YOLO: Challenges, architectural successors, datasets and applications. 2022: p. 1-33. Li, G., et al. Knowledge distillation for object detection via rank mimicking and prediction- guided feature imitation. in Proceedings of the AAAI Conference on Artificial Intelligence. 2022. 19.\\n\\n14.\\n\\n15.\\n\\n16.', metadata={'source': 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/2304.05295v1.pdf'})]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_per_chunk = 100\n",
    "max_words_final = 500\n",
    "\n",
    "initial_summaries = []\n",
    "for chunk in chunks:\n",
    "    summary = summarize_text(chunk.page_content, max_words_per_chunk)\n",
    "    initial_summaries.append(summary)\n",
    "\n",
    "combined_summaries = \"\\n\\n\".join(initial_summaries)\n",
    "final_summary = summarize_text(combined_summaries, max_words_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a 500-word summary of the text:\n",
      "\n",
      "Object detection is a crucial task in computer vision, aiming to recognize and localize objects in images andvideos with high accuracy. Recent advancements in deep learning and Convolutional Neural Networks (CNNs) have improved object detection performance in benchmark datasets. However, object detection in unconstrained environments remains challenging due to variations in lighting, viewpoint, and cluttered backgrounds.\n",
      "\n",
      "The paper discusses various challenges, datasets, and state-of-the-art approaches for object detection. Region-based object detectors, such as R-CNN, generate region proposals using selective search algorithms and classify them using a Support Vector Machine (SVM). However, R-CNN has limitations, including slow training and inference times. To address these issues, variants such as Fast R-CNN and Faster R-CNN were proposed.\n",
      "\n",
      "The paper also discusses the importance of datasets, such as PASCAL VOC, ImageNet, COCO, and Open Images, which provide detailed annotations, including bounding box and segmentation mask annotations. These datasets allow researchers to test object detection techniques on different scenarios, showcasing the diversity in size, number of classes, and annotation types.\n",
      "\n",
      "State-of-the-art object detection techniques are categorized into two-stage detectors and single-stage detectors. Two-stage detectors consist of a region proposal stage followed by a classification stage, while single-stage detectors predict object bounding boxes and class probabilities directly from an image. Examples of two-stage detectors include R-CNN and Fast R-CNN, while single-stage detectors include YOLO, SSD, and Retina Net.\n",
      "\n",
      "The paper also discusses the challenges of object detection, including illumination changes, viewpoint variation, and occlusion. To overcome these issues, researchers have proposed various techniques, including color constancy, viewpoint invariant features, and part-based models.\n",
      "\n",
      "The performance of various object detection techniques is compared, highlighting strengths and weaknesses. Despite progress, object detection remains challenging, and future research directions include developing more robust algorithms, efficient small-scale object detection, and integration with other sensor modalities.\n",
      "\n",
      "The paper concludes by emphasizing the importance of object detection in various applications, including autonomous vehicles, robotics, and surveillance systems. By developing more robust and efficient object detection algorithms, researchers can improve object detection, enabling more reliable and efficient applications in these areas.\n"
     ]
    }
   ],
   "source": [
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
