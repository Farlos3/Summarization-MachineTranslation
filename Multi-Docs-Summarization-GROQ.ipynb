{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = \"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\"\n",
    "client = Groq(api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_words, retries=3):\n",
    "    prompt = f\"Summarize the following text in approximately {max_words} words. It's crucial to keep the summary as close to {max_words} words as possible:\\n\\n{text}\\n\\nSummary:\"\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant that provides concise summaries. Always aim to keep your summaries as close to the requested word count as possible.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        temperature=0.7,\n",
    "        top_p=1,\n",
    "        max_tokens=max_words * 2\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content(base_path, pdf_file):\n",
    "    full_path = os.path.join(base_path, pdf_file)\n",
    "    with open(full_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + '\\n'\n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter_chunks(text, filename):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    langchain_doc = Document(page_content=text, metadata={\"source\": filename})\n",
    "    return text_splitter.split_documents([langchain_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document1.pdf...\n",
      "Processing document2.pdf...\n",
      "Processing document3.pdf...\n",
      "Total number of chunks across all documents: 18\n"
     ]
    }
   ],
   "source": [
    "base_path = 'F:/Super AI SS4/AI-builder/Large-Language-Models-(LLMs)/try-it/Text-Summarization/Dataset/'\n",
    "pdf_files = ['document1.pdf', 'document2.pdf', 'document3.pdf']\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"Processing {pdf_file}...\")\n",
    "    content = extract_pdf_content(base_path, pdf_file)\n",
    "    chunks = splitter_chunks(content, pdf_file)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total number of chunks across all documents: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "documents = []\n",
    "\n",
    "for i, pdf_file in enumerate(pdf_files, 1):\n",
    "    content = extract_pdf_content(base_path, pdf_file)\n",
    "    document = {\n",
    "        'id': i,\n",
    "        'filename': pdf_file,\n",
    "        'content': content,\n",
    "        'length': len(content)\n",
    "    }\n",
    "    documents.append(document)\n",
    "\n",
    "for doc in documents:\n",
    "    print(f\"Document ID: {doc['id']}\")\n",
    "    print(f\"Filename: {doc['filename']}\")\n",
    "    print(f\"Content length: {doc['length']} characters\")\n",
    "    print(f\"Preview: {doc['content'][:100]}...\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing chunk 1/18...\n",
      "Summarizing chunk 2/18...\n",
      "Summarizing chunk 3/18...\n",
      "Summarizing chunk 4/18...\n",
      "Summarizing chunk 5/18...\n",
      "Summarizing chunk 6/18...\n",
      "Summarizing chunk 7/18...\n",
      "Summarizing chunk 8/18...\n",
      "Summarizing chunk 9/18...\n",
      "Summarizing chunk 10/18...\n",
      "Summarizing chunk 11/18...\n",
      "Summarizing chunk 12/18...\n",
      "Summarizing chunk 13/18...\n",
      "Summarizing chunk 14/18...\n",
      "Summarizing chunk 15/18...\n",
      "Summarizing chunk 16/18...\n",
      "Summarizing chunk 17/18...\n",
      "Summarizing chunk 18/18...\n"
     ]
    }
   ],
   "source": [
    "chunk_summaries = []\n",
    "\n",
    "for i, chunk in enumerate(all_chunks, 1):\n",
    "    print(f\"Summarizing chunk {i}/{len(all_chunks)}...\")\n",
    "    summary = summarize_text(chunk.page_content, 100)\n",
    "    if summary:\n",
    "        chunk_summaries.append(summary)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate change, primarily caused by human activities since the industrial revolution, involves long-term shifts in temperature and weather patterns due to increased greenhouse gases, particularly carbon dioxide. These shifts impact Earth's energy balance, causing effects like polar ice melting, sea level rises, and extreme weather events. Coastal communities are at risk of flooding and erosion, while agriculture faces challenges from changing weather patterns, affecting crop yields and food security. Climate change intensifies heatwaves, hurricanes, and wildfires, threatening ecosystems and biodiversity. Ocean acidification endangers marine life, especially coral reefs and shellfish.\n",
      "\n",
      "The international community has recognized the urgency of this issue, leading to agreements like the Paris Agreement, which aims to limit warming. Mitigation strategies involve reducing greenhouse gas emissions, while adaptation strategies, like managing risks and improving resilience, are essential for managing climate change impacts. Climate change significantly impacts ecosystems, biodiversity, and natural resources worldwide, causing species extinction, altered biological events, and invasive species expansion.\n",
      "\n",
      "Climate change has far-reaching and complex economic consequences, affecting various sectors, including agriculture, energy, infrastructure, tourism, real estate, and finance. Transitioning to a low-carbon economy is necessary but brings challenges and opportunities. Adapting to climate change involves significant costs, particularly for developing nations, and may increase global economic inequalities. Despite these challenges, addressing climate change offers economic opportunities, such as new industries, technologies, and jobs. This comprehensive, global action is urgently needed to mitigate warming and adapt to changes already happening.\n"
     ]
    }
   ],
   "source": [
    "combined_summaries = \"\\n\\n\".join(chunk_summaries)\n",
    "final_summary = summarize_text(combined_summaries, 250)\n",
    "\n",
    "print(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
